{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNunxD9jaC2zmDyiqkkwMGS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-sahu/jax-sharding-tutorials/blob/main/Shard_Map_Detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade jax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqAONEiCCD6O",
        "outputId": "dab8fb01-260c-4564-c9d9-e9ac214b8133"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: jaxlib<=0.6.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.6.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.15.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Grand Shard_map Tutorial: A Wizard's Guide to Parallel Spells üßô‚Äç‚ôÇÔ∏èüìú‚ú®\n",
        "\n",
        "Welcome, young Data Wizards! Archmage Jaxus here, your guide on this quest to master the **Scroll of Sharding**, known in the common tongue as `jax.shard_map`. You've learned individual spells (`jax.jit`, basic functions), but some Magic Scrolls (datasets, models) are too immense for a single wizard. `shard_map` is the ancient art of **manual parallelism**, where a coven of wizards (your devices) work in concert, each tackling a fragment of the Great Scroll, and communicating through **explicit collective spells**.\n",
        "\n",
        "It's more hands-on than the \"Auto-Sort Spell\" (`jit` with automatic partitioning), which often does a good job but doesn't always know the specific enchantments you wish to cast. And it's an evolution of the \"Elder Scroll of Mapping\" (`pmap`), offering more power, flexibility, and even the ability to debug your spells eagerly before committing them to the aether (compiling them).\n",
        "\n",
        "Let's begin by preparing our School of Magic (Python environment) and summoning our apprentice wizards (simulating 8 devices for this tutorial)."
      ],
      "metadata": {
        "id": "n8KXhjc5AV7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEOrX2ax_DgY",
        "outputId": "f80d493d-eb1a-4ab1-f33e-bc26d6284db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.1 is installed, but it is not compatible with the installed jaxlib version 0.6.0, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome, Wizards! We have 8 apprentices ready for their first lesson.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Ensure we have 8 \"apprentice wizards\" (CPU devices) for our spells.\n",
        "# For this tutorial, we'll force JAX to see 8 CPU devices.\n",
        "# In a real scenario, these would be your actual hardware devices (CPUs, GPUs, TPUs).\n",
        "# IMPORTANT: Run this cell *before* importing JAX for the first time in a session if you're running locally.\n",
        "# If you've already imported JAX, you might need to restart your kernel.\n",
        "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np # Standard numpy for some array creations\n",
        "from functools import partial\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "from jax.tree_util import tree_map, tree_all\n",
        "from jax import lax # For collective operations and other utilities\n",
        "\n",
        "# A little helper spell to check if our distributed magic matches single-wizard results\n",
        "def allclose(a, b, atol=1e-2, rtol=1e-2): # Using a slightly looser tolerance for tutorial examples\n",
        "  \"\"\"Checks if all elements in two pytrees are close.\"\"\"\n",
        "  return tree_all(tree_map(partial(jnp.allclose, atol=atol, rtol=rtol), a, b))\n",
        "\n",
        "print(f\"Welcome, Wizards! We have {len(jax.devices())} apprentices ready for their first lesson.\")\n",
        "if len(jax.devices()) != 8:\n",
        "    print(\"\\n‚ö†Ô∏è Warning: Could not simulate 8 devices as expected.\")\n",
        "    print(\"The Archmage's incantation for device simulation might have been resisted, or you might be running in an environment where this flag is overridden.\")\n",
        "    print(\"You can still follow the tutorial, but visualizations and sharding specifics might differ slightly.\")\n",
        "    print(\"The core principles of the shard_map spell remain the same!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 1: The First Sharding Spell - `matmul_basic` üí•\n",
        "\n",
        "Our first group spell will be a fundamental one: multiplying two Magic Scrolls (matrices). To do this, Archmage Jaxus explains, \"We must first define how our wizards are arranged for collaborative spellcasting. This arrangement is known as a **`Mesh`**. Think of it as assigning each wizard a specific workstation in a magical grid.\"\n",
        "\n",
        "We'll tell JAX how many wizards are in each dimension of this grid and give names to these dimensions. For instance, we could have a 2D grid."
      ],
      "metadata": {
        "id": "wd7klLyxCi1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's arrange our 8 wizards in a 4x2 formation.\n",
        "# We give names to these dimensions: 'row_of_wizards' and 'col_of_wizards'.\n",
        "mesh_shape = (4, 2)  # 4 rows of wizards, 2 columns of wizards\n",
        "mesh_axis_names = ('row_of_wizards', 'col_of_wizards') # Naming the dimensions of our magical grid\n",
        "\n",
        "# Create the mesh\n",
        "wizard_mesh = jax.make_mesh(mesh_shape, mesh_axis_names)\n",
        "\n",
        "print(f\"Our wizarding mesh configuration: {wizard_mesh}\")\n",
        "print(f\"Total wizards in this mesh: {wizard_mesh.size}\")\n",
        "print(f\"Wizardry is organized along these axes: {wizard_mesh.axis_names}\")\n",
        "print(f\"Number of wizards in the '{wizard_mesh.axis_names[0]}' dimension: {wizard_mesh.shape[wizard_mesh.axis_names[0]]}\")\n",
        "print(f\"Number of wizards in the '{wizard_mesh.axis_names[1]}' dimension: {wizard_mesh.shape[wizard_mesh.axis_names[1]]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgJxgFq7CV0d",
        "outputId": "011c66ce-5050-40e5-91f9-1ba6ad9bb387"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our wizarding mesh configuration: Mesh('row_of_wizards': 4, 'col_of_wizards': 2, axis_types=(Auto, Auto))\n",
            "Total wizards in this mesh: 8\n",
            "Wizardry is organized along these axes: ('row_of_wizards', 'col_of_wizards')\n",
            "Number of wizards in the 'row_of_wizards' dimension: 4\n",
            "Number of wizards in the 'col_of_wizards' dimension: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Excellent!\" exclaims Archmage Jaxus. \"With our wizards arranged in the `wizard_mesh`, we now need to tell them how to divide the work and how to combine their results. For this, we use **Partitioning Runes**, known as `PartitionSpec` (which we'll call `P` for short). These runes are part of the `in_specs` (how to divide input scrolls) and `out_specs` (how to assemble the final scroll).\"\n",
        "\n",
        "* **`in_specs`**: A set of runes, one for each input scroll. Each rune specifies how that scroll's dimensions are sharded (split) across the named axes of our `wizard_mesh`.\n",
        "    * For example, `P('row_of_wizards', 'col_of_wizards')` means the scroll's first dimension is split among the 'row_of_wizards' and its second dimension among the 'col_of_wizards'.\n",
        "    * If a rune uses `None` for a scroll dimension, that dimension is *not* split along any *additional* mesh axis beyond what other parts of the spec might imply for other scroll axes. Each wizard receives a full slice of that dimension.\n",
        "* **`out_specs`**: Similar runes for the output scroll, dictating how the pieces computed by each wizard are reassembled.\n",
        "\n",
        "Let's prepare two Magic Scrolls (matrices `scroll_A` and `scroll_B`) and define our first sharded matrix multiplication spell."
      ],
      "metadata": {
        "id": "C4TAMTL1C5-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As of now jax.shard_map does not work\n",
        "from jax.experimental.shard_map import shard_map\n",
        "\n",
        "# Our two Magic Scrolls (matrices) to be multiplied\n",
        "# scroll_A: 8x16, scroll_B: 16x4. Result should be 8x4.\n",
        "scroll_A = jnp.arange(8 * 16., dtype=jnp.float32).reshape(8, 16)\n",
        "scroll_B = jnp.arange(16 * 4., dtype=jnp.float32).reshape(16, 4)\n",
        "\n",
        "# The Sharding Spell for matrix multiplication\n",
        "# We use the 'wizard_mesh' (4 rows, 2 columns of wizards) defined earlier.\n",
        "@partial(shard_map,\n",
        "         mesh=wizard_mesh,\n",
        "         in_specs=(P('row_of_wizards', 'col_of_wizards'), P('col_of_wizards', None)), # How scroll_A and scroll_B are sharded\n",
        "         out_specs=P('row_of_wizards', None)) # How the result scroll_C is sharded\n",
        "def matmul_basic_sharded(scroll_A_fragment, scroll_B_fragment):\n",
        "  # scroll_A is 8x16. Sharded by ('row_of_wizards'(4), 'col_of_wizards'(2)).\n",
        "  # So, scroll_A_fragment for each wizard will have shape (8/4, 16/2) = (2, 8).\n",
        "\n",
        "  # scroll_B is 16x4. Sharded by ('col_of_wizards'(2), None).\n",
        "  # So, scroll_B_fragment for each wizard will have shape (16/2, 4) = (8, 4).\n",
        "  # The 'None' means its second dimension (size 4) is not further sharded by 'row_of_wizards'. Wizards in the same 'col_of_wizards'\n",
        "  # but different 'row_of_wizards' will get different scroll_A_fragments but the *same* relevant scroll_B_fragment if we consider\n",
        "  # how data is split. However, for this specific PartitionSpec, each 'col_of_wizards' group gets its unique (8,4) slice of B.\n",
        "  print(f\"Each wizard received A_frag: {scroll_A_fragment.shape}, B_frag: {scroll_B_fragment.shape}\")\n",
        "\n",
        "  # Each wizard computes their part of the sum (local dot product)\n",
        "  partial_sum_C = jnp.dot(scroll_A_fragment, scroll_B_fragment)\n",
        "  # partial_sum_C will be (2,4) for each wizard\n",
        "\n",
        "  # Now, for the magic of combining results!\n",
        "  # Wizards who are in the same 'row_of_wizards' but different 'col_of_wizards' worked on different\n",
        "  # parts of the original scroll_A's columns and scroll_B's rows (the contracting dimension).\n",
        "  # We need to sum their partial_sum_C contributions.\n",
        "  # The 'col_of_wizards' mesh axis was used for this split, so we sum over it.\n",
        "  final_C_fragment_for_row = jax.lax.psum(partial_sum_C, axis_name='col_of_wizards')\n",
        "  # final_C_fragment_for_row is still (2,4) but now contains the sum over 'col_of_wizards' for that row.\n",
        "\n",
        "  return final_C_fragment_for_row\n",
        "\n",
        "print(\"Defining the 'matmul_basic_sharded' spell. Prints will appear when it's first cast (traced).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uwaG-E9Czsv",
        "outputId": "57c114df-0c2a-41ef-acb0-098b6bdbd4e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining the 'matmul_basic_sharded' spell. Prints will appear when it's first cast (traced).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cast the spell!\n",
        "# JAX automatically handles the distribution of scroll_A and scroll_B fragments\n",
        "# to the wizards according to in_specs.\n",
        "print(\"Casting matmul_basic_sharded for the first time (with prints from inside the spell):\")\n",
        "scroll_C_sharded = matmul_basic_sharded(scroll_A, scroll_B)\n",
        "\n",
        "# Casting it again (prints from inside the spell will likely be JIT-compiled away or not re-traced)\n",
        "# print(\"\\nCasting again (prints from inside might be optimized away):\")\n",
        "# scroll_C_sharded_again = matmul_basic_sharded(scroll_A, scroll_B)\n",
        "\n",
        "print(f\"\\nShape of original scroll_A: {scroll_A.shape}\")\n",
        "print(f\"Shape of original scroll_B: {scroll_B.shape}\")\n",
        "print(f\"Shape of the sharded result scroll_C: {scroll_C_sharded.shape}\") # Expected: (8, 4)\n",
        "\n",
        "# Let's look at a piece of the output\n",
        "print(f\"A peek at the resulting scroll_C_sharded (first 2 rows):\\n{scroll_C_sharded[:2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkaC5607DKvP",
        "outputId": "c43a485f-e0b9-4547-90ea-ee0dcdb30c74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Casting matmul_basic_sharded for the first time (with prints from inside the spell):\n",
            "Each wizard received A_frag: (2, 8), B_frag: (8, 4)\n",
            "\n",
            "Shape of original scroll_A: (8, 16)\n",
            "Shape of original scroll_B: (16, 4)\n",
            "Shape of the sharded result scroll_C: (8, 4)\n",
            "A peek at the resulting scroll_C_sharded (first 2 rows):\n",
            "[[ 4960.  5080.  5200.  5320.]\n",
            " [12640. 13016. 13392. 13768.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Observe!\" Archmage Jaxus gestures towards the glowing result. \"Each wizard performed a local calculation on their fragments. Then, they used a crucial collective spell: `jax.lax.psum` (Parallel Sum). This spell instructed wizards sharing the same 'row_of_wizards' to sum their individual `partial_sum_C` pieces along the 'col_of_wizards' dimension. This is how the full dot product is correctly calculated across the distributed pieces.\"\n",
        "\n",
        "The `out_specs=P('row_of_wizards', None)` then tells JAX that the final `scroll_C_sharded` should be formed by taking the (2x4) `final_C_fragment_for_row` from each of the 4 'row_of_wizards' groups and concatenating them along the first dimension, resulting in the final 8x4 scroll. The `None` for the second dimension indicates that the (already correctly summed) second dimension of the fragments is used as is.\n",
        "\n",
        "\"But is our collective magic true?\" he ponders. \"We must always verify our sharded spells against the work of a single, focused wizard.\""
      ],
      "metadata": {
        "id": "Qmca_ZtOEj8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single wizard computation for reference\n",
        "scroll_C_single_wizard = jnp.dot(scroll_A, scroll_B)\n",
        "\n",
        "print(f\"A peek at the single wizard's scroll_C (first 2 rows):\\n{scroll_C_single_wizard[:2]}\")\n",
        "\n",
        "# Verify our sharded spell against the single wizard's result\n",
        "verification_passed = allclose(scroll_C_sharded, scroll_C_single_wizard)\n",
        "if verification_passed:\n",
        "    print(f\"\\nSpell Succeeded! Our sharded spell matches the single wizard's calculation. ‚ú®\")\n",
        "else:\n",
        "    print(f\"\\nSpell Miscast! Our sharded spell does NOT match the single wizard. Check the incantations! ‚ùå\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0BpBBxHDwEE",
        "outputId": "164c231d-cdbc-4075-b951-681502513ed6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A peek at the single wizard's scroll_C (first 2 rows):\n",
            "[[ 4960.  5080.  5200.  5320.]\n",
            " [12640. 13016. 13392. 13768.]]\n",
            "\n",
            "Spell Succeeded! Our sharded spell matches the single wizard's calculation. ‚ú®\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Indeed, our magic holds true!\" declares Archmage Jaxus. \"Now, let's peer into the Crystal Ball of `visualize_array_sharding`. This magical tool allows us to see precisely how our `scroll_C_sharded` is distributed across the wizarding workstations defined by our `wizard_mesh`.\"\n",
        "\n",
        "This visualization will confirm that the output `scroll_C` (8x4) is sharded along its first dimension ('row_of_wizards', which has a size of 4 from our 4x2 mesh). Each of the 4 groups of wizards along this 'row_of_wizards' dimension holds a 2x4 piece of the final scroll. The `None` in `out_specs=P('row_of_wizards', None)` for the second dimension of the scroll meant it was not further split by the 'col_of_wizards' mesh axis during output assembly, as the `psum` had already ensured values were correctly combined along that dimension."
      ],
      "metadata": {
        "id": "mlsM-kHdFryU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sharding of the resulting Scroll C (scroll_C_sharded):\")\n",
        "jax.debug.visualize_array_sharding(scroll_C_sharded)\n",
        "\n",
        "# To understand what shard_map's `in_specs` effectively does to the inputs,\n",
        "# we can manually shard them using jax.device_put with NamedSharding.\n",
        "# This is conceptually what shard_map prepares for its mapped function.\n",
        "print(\"\\nFor context, let's visualize how scroll_A would be sharded based on its in_spec P('row_of_wizards', 'col_of_wizards'):\")\n",
        "sharded_A_for_visualization = jax.device_put(scroll_A, NamedSharding(wizard_mesh, P('row_of_wizards', 'col_of_wizards')))\n",
        "jax.debug.visualize_array_sharding(sharded_A_for_visualization)\n",
        "\n",
        "print(\"\\nAnd how scroll_B would be sharded based on its in_spec P('col_of_wizards', None):\")\n",
        "sharded_B_for_visualization = jax.device_put(scroll_B, NamedSharding(wizard_mesh, P('col_of_wizards', None)))\n",
        "jax.debug.visualize_array_sharding(sharded_B_for_visualization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "dxXbV7O7FLSG",
        "outputId": "3d304de1-12e0-40c7-a9b3-430a30ab40ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sharding of the resulting Scroll C (scroll_C_sharded):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 4,5\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 6,7\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0,1   </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 2,3   </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  CPU 4,5   </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  CPU 6,7   </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For context, let's visualize how scroll_A would be sharded based on its in_spec P('row_of_wizards', 'col_of_wizards'):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m          \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m                         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m          \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m          \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m          \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;140;162;82m                         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m                         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                         \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m          \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m          \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m          \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m          \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m                         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m                         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m          \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m          \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m          \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;165;81;148m                         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m                         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          CPU 0          </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">          CPU 1          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">          CPU 2          </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">          CPU 3          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                         </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">          CPU 4          </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">          CPU 5          </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">          CPU 6          </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">          CPU 7          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And how scroll_B would be sharded based on its in_spec P('col_of_wizards', None):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,2,4,6\u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121mCPU 1,3,5,7\u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">CPU 0,2,4,6</span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">CPU 1,3,5,7</span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualizations above show how `shard_map` distributes data based on the `Mesh` and `PartitionSpec` you provide. This gives you explicit control over data layout and computation.\n",
        "\n",
        "This contrasts with `jax.jit`'s automatic parallelization, where the JAX compiler attempts to figure out an efficient sharding strategy for you behind the scenes. With `shard_map`, you are the one defining the parallel execution plan. Both approaches are valuable: `jit` for automatic optimization and `shard_map` for fine-grained manual control when needed."
      ],
      "metadata": {
        "id": "_x-eZarxGE5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.sharding import AxisType\n",
        "from jax.experimental.shard import reshard, auto_axes\n",
        "\n",
        "@jax.jit\n",
        "def matmul(a, b):\n",
        "  return jnp.dot(a, b)\n",
        "\n",
        "scroll_A_auto = jax.device_put(scroll_A, NamedSharding(wizard_mesh, P('row_of_wizards', 'col_of_wizards')))\n",
        "scroll_B_auto = jax.device_put(scroll_B, NamedSharding(wizard_mesh, P('col_of_wizards', None)))\n",
        "\n",
        "print(f\"scroll_A_auto: {jax.typeof(scroll_A_auto)}\")\n",
        "print(f\"scroll_B_auto: {jax.typeof(scroll_B_auto)}\")\n",
        "\n",
        "scroll_C_auto = matmul(scroll_A_auto, scroll_B_auto)\n",
        "print(f\"scroll_C_Auto: {jax.typeof(scroll_C_auto)}\")\n",
        "\n",
        "# Let's look at a piece of the output\n",
        "print(f\"A peek at the resulting scroll_C_auto (first 2 rows):\\n{scroll_C_auto[:2]}\")\n",
        "\n",
        "# Use auto_axes for now as malmul operation is not supported in explicit axis yet.\n",
        "@auto_axes\n",
        "def matmul_explicit(a, b):\n",
        "  return jnp.dot(a, b)\n",
        "\n",
        "# Explicit Sharding\n",
        "wizard_mesh_explicit = jax.make_mesh(mesh_shape, mesh_axis_names, axis_types=(AxisType.Explicit, AxisType.Explicit))\n",
        "scroll_A_explicit = jax.device_put(scroll_A, NamedSharding(wizard_mesh_explicit, P('row_of_wizards', 'col_of_wizards')))\n",
        "scroll_B_explicit = jax.device_put(scroll_B, NamedSharding(wizard_mesh_explicit, P('col_of_wizards', None)))\n",
        "\n",
        "print(f\"scroll_A_explicit: {jax.typeof(scroll_A_explicit)}\")\n",
        "print(f\"scroll_B_explicit: {jax.typeof(scroll_B_explicit)}\")\n",
        "\n",
        "scroll_C_explicit = matmul_explicit(scroll_A_explicit, scroll_B_explicit, out_shardings=NamedSharding(wizard_mesh_explicit, P('row_of_wizards', None)))\n",
        "print(f\"scroll_C_explicit: {jax.typeof(scroll_C_explicit)}\")\n",
        "\n",
        "# Let's look at a piece of the output\n",
        "print(f\"A peek at the resulting scroll_C_explicit (first 2 rows):\\n{scroll_C_explicit[:2]}\")\n",
        "print(f\"Is all restuls equal? {allclose(scroll_C_sharded, scroll_C_auto)=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyl25SxtGh7I",
        "outputId": "5f392a1f-f6d6-491f-c3c1-139a7e39aa96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scroll_A_auto: ShapedArray(float32[8,16])\n",
            "scroll_B_auto: ShapedArray(float32[16,4])\n",
            "scroll_C_Auto: ShapedArray(float32[8,4])\n",
            "A peek at the resulting scroll_C_auto (first 2 rows):\n",
            "[[ 4960.  5080.  5200.  5320.]\n",
            " [12640. 13016. 13392. 13768.]]\n",
            "scroll_A_explicit: ShapedArray(float32[8@row_of_wizards,16@col_of_wizards])\n",
            "scroll_B_explicit: ShapedArray(float32[16@col_of_wizards,4])\n",
            "scroll_C_explicit: ShapedArray(float32[8@row_of_wizards,4])\n",
            "A peek at the resulting scroll_C_explicit (first 2 rows):\n",
            "[[ 4960.  5080.  5200.  5320.]\n",
            " [12640. 13016. 13392. 13768.]]\n",
            "Is all restuls equal? allclose(scroll_C_sharded, scroll_C_auto)=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding How Maps Handle Array Dimensions\n",
        "\n",
        "JAX provides different functions for mapping operations over arrays, like `jax.vmap` and `jax.shard_map`. A key difference between them is how they treat the dimensions (or rank) of the arrays they operate on.\n",
        "\n",
        "Let's first look at `jax.vmap`. It's often described as a **rank-reducing map**."
      ],
      "metadata": {
        "id": "EF0bmyqSGVnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: jax.vmap\n",
        "# Suppose we have a function that processes a single matrix (2D array)\n",
        "def process_matrix(matrix):\n",
        "  # Example operation: sum its rows\n",
        "  return jnp.sum(matrix, axis=1)\n",
        "\n",
        "# Now, let's say we have a stack of matrices (a 3D array)\n",
        "# e.g., 4 matrices, each of size 2x3\n",
        "stacked_matrices = jnp.arange(4 * 2 * 3.).reshape(4, 2, 3)\n",
        "print(f\"Original stacked_matrices shape: {stacked_matrices.shape} (rank 3)\")\n",
        "\n",
        "# When we use vmap to map process_matrix over the first axis (the stack of 4 matrices):\n",
        "# in_axes=0 means map over the 0-th axis of stacked_matrices.\n",
        "# The 'process_matrix' function will receive individual 2x3 matrices (rank 2).\n",
        "vmapped_output = jax.vmap(process_matrix, in_axes=0)(stacked_matrices)\n",
        "\n",
        "# If each 2x3 matrix results in a (2,) vector (sum of 2 rows),\n",
        "# and we have 4 such matrices, the output will be 4x2.\n",
        "print(f\"vmapped_output shape: {vmapped_output.shape}\")\n",
        "\n",
        "# The number of logical applications of 'process_matrix' is determined\n",
        "# by the size of the input axis being mapped over (here, 4)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZQi_dOuFvAg",
        "outputId": "be47a6f5-bcd2-4bd8-e8ef-8b16bea6c533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original stacked_matrices shape: (4, 2, 3) (rank 3)\n",
            "vmapped_output shape: (4, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast, `jax.shard_map` is a **rank-preserving map**.\n",
        "\n",
        "When you use `shard_map`, the function you define operates on blocks (shards) of the input arrays. These blocks retain the same rank as the original input arrays. The number of logical applications of your function is determined by the total size of the `Mesh` you define, not by the size of any particular input array axis."
      ],
      "metadata": {
        "id": "Qc_DcM2q5tGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: jax.shard_map\n",
        "# Let's use a simple 1D mesh of 4 devices for this example.\n",
        "simple_mesh_1d = Mesh(np.array(jax.devices()[:4]), ('data_parallel_dim',))\n",
        "\n",
        "# An 8x5 array (rank 2)\n",
        "big_array = jnp.arange(8 * 5.).reshape(8, 5)\n",
        "print(f\"Original big_array shape: {big_array.shape} (rank 2)\")\n",
        "\n",
        "# The function to be mapped by shard_map\n",
        "# It will receive a block of 'big_array'.\n",
        "@partial(shard_map,\n",
        "         mesh=simple_mesh_1d,\n",
        "         in_specs=P('data_parallel_dim', None), # Shard 0-th axis of big_array across 'data_parallel_dim' (size 4)\n",
        "         out_specs=P('data_parallel_dim', None))\n",
        "def process_block(array_block):\n",
        "  # If big_array is (8,5) and mesh has 4 devices along 'data_parallel_dim',\n",
        "  # each array_block will be (8/4, 5) = (2,5). Its rank is still 2.\n",
        "  print(f\"  Inside shard_map: process_block received block of shape: {array_block.shape}\")\n",
        "  # Example operation: sum its rows, result will be (2,) for each block\n",
        "  return jnp.sum(array_block, axis=1)\n",
        "\n",
        "# Execute with shard_map\n",
        "sharded_output = process_block(big_array)\n",
        "\n",
        "# Each of the 4 devices produces a (2,) vector.\n",
        "# out_specs=P('data_parallel_dim', None) concatenates these along the first dimension.\n",
        "# So, final output shape is (4*2,) = (8,).\n",
        "print(f\"sharded_output shape: {sharded_output.shape}\")\n",
        "\n",
        "# The number of logical applications of 'process_block' is determined\n",
        "# by the mesh size (here, 4)."
      ],
      "metadata": {
        "id": "txTznZqrGdfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98976497-d9d7-4a1f-9649-6da027035f52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original big_array shape: (8, 5) (rank 2)\n",
            "  Inside shard_map: process_block received block of shape: (2, 5)\n",
            "sharded_output shape: (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the key distinction:\n",
        "* `vmap`: Reduces rank for the mapped function; number of instances depends on input axis size.\n",
        "* `shard_map`: Preserves rank for the mapped function; number of instances depends on mesh size."
      ],
      "metadata": {
        "id": "cbi_JT6R6BS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding How Mappings Affect Array Rank\n",
        "\n",
        "JAX provides different ways to map functions over data. A key distinction is how they handle the \"rank\" (number of dimensions) of arrays.\n",
        "\n",
        "**`jax.vmap`: The Rank-Reducing Map**\n",
        "\n",
        "Think of `jax.vmap` as a way to automatically \"vectorize\" a function. If you have a function that works on a single item (e.g., a vector), `vmap` can make it work on a batch of items (e.g., a matrix where each row is a vector).\n",
        "\n",
        "When `vmap` applies your function, the function receives inputs with one less dimension than what `vmap` was given (the dimension being mapped over is \"removed\" for the function's view). The number of times your function is logically applied is determined by the size of this mapped axis."
      ],
      "metadata": {
        "id": "KM6di-M-6GBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vmap example:\n",
        "# This function expects a 1D array (vector)\n",
        "def process_vector(vector):\n",
        "  return jnp.sum(vector * 2)\n",
        "\n",
        "# A 2D array (a batch of 3 vectors, each of size 4)\n",
        "batched_vectors = jnp.arange(3 * 4.).reshape(3, 4)\n",
        "print(f\"Original batched_vectors shape: {batched_vectors.shape}\")\n",
        "\n",
        "# Map process_vector over the 0-th axis (the batch dimension)\n",
        "# process_vector will receive individual 1D arrays of shape (4,).\n",
        "vmapped_result = jax.vmap(process_vector, in_axes=0, out_axes=0)(batched_vectors)\n",
        "\n",
        "print(f\"vmapped_result: {vmapped_result}\")\n",
        "print(f\"vmapped_result shape: {vmapped_result.shape}\") # Shape (3,) - one result per input vector\n",
        "\n",
        "# Reference: what vmap does semantically\n",
        "expected_vmap_result = jnp.array([process_vector(vector) for vector in batched_vectors])\n",
        "print(f\"Does vmap work as expected? {allclose(vmapped_result, expected_vmap_result)}\")\n",
        "print(f\"Number of logical applications of process_vector: {batched_vectors.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXvLT-5A51K4",
        "outputId": "711e71de-488e-4bf6-a348-2ef4c0a33f02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original batched_vectors shape: (3, 4)\n",
            "vmapped_result: [12. 44. 76.]\n",
            "vmapped_result shape: (3,)\n",
            "Does vmap work as expected? True\n",
            "Number of logical applications of process_vector: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`jax.shard_map`: The Rank-Preserving Map**\n",
        "\n",
        "In contrast, `jax.shard_map` does *not* reduce the rank of the arrays for the function it maps. Your function receives \"shards\" or \"blocks\" of the input arrays that have the *same rank* as the original global inputs.\n",
        "\n",
        "The number of logical applications of your function in `shard_map` is determined by the total size of the `Mesh` you define (e.g., a 2x2 mesh means 4 logical applications), not by the size of any particular input array axis."
      ],
      "metadata": {
        "id": "deqFTveS6tAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shard_map example:\n",
        "# Let's use a simple 1D mesh of 2 devices for this.\n",
        "simple_mesh_1d = Mesh(np.array(jax.devices()[:2]), ('data_parallel_axis',))\n",
        "\n",
        "# An 8x4 array\n",
        "large_array = jnp.arange(8 * 4.).reshape(8, 4)\n",
        "print(f\"Original large_array shape: {large_array.shape}\")\n",
        "\n",
        "# This function will receive a block of large_array\n",
        "@partial(shard_map,\n",
        "         mesh=simple_mesh_1d,\n",
        "         in_specs=P('data_parallel_axis'), # Shard the 0-th axis of large_array\n",
        "         out_specs=P('data_parallel_axis'))\n",
        "def process_block_shmap(array_block):\n",
        "  # If large_array is (8,4) and mesh has 2 devices along 'data_parallel_axis',\n",
        "  # array_block will be (8/2, 4) = (4,4). Rank is preserved!\n",
        "  print(f\"  Inside shard_map: process_block_shmap received block of shape: {array_block.shape}\")\n",
        "  return jnp.sum(array_block, axis=1) # Example: sum each row, result (4,) per block\n",
        "\n",
        "print(\"Running shard_map example:\")\n",
        "shmap_result = process_block_shmap(large_array)\n",
        "\n",
        "# Expected output shape:\n",
        "# Each of the 2 devices produces a (4,) result.\n",
        "# out_specs=P('data_parallel_axis') concatenates these along the 0-th axis.\n",
        "# So, (2*4,) = (8,).\n",
        "print(f\"\\nshmap_result: {shmap_result}\")\n",
        "print(f\"shmap_result shape: {shmap_result.shape}\")\n",
        "print(f\"Number of logical applications of process_block_shmap: {simple_mesh_1d.size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylu0tU6U6VjO",
        "outputId": "9e13be6e-571a-4ef5-84bb-adae159a7595"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original large_array shape: (8, 4)\n",
            "Running shard_map example:\n",
            "  Inside shard_map: process_block_shmap received block of shape: (4, 4)\n",
            "\n",
            "shmap_result: [  6.  22.  38.  54.  70.  86. 102. 118.]\n",
            "shmap_result shape: (8,)\n",
            "Number of logical applications of process_block_shmap: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary:\n",
        "* `vmap`: Reduces rank for the mapped function; number of instances = size of mapped input axis.\n",
        "* `shard_map`: Preserves rank for the mapped function; number of instances = size of the `Mesh`.\n",
        "\n",
        "This rank-preserving nature is key to how `shard_map` allows you to write code that operates on local blocks of a larger, distributed array, while still reasoning about the full dimensionality of those blocks."
      ],
      "metadata": {
        "id": "m5YOi1UJ7A5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning Data Distribution: `in_specs`\n",
        "\n",
        "The `in_specs` argument to `shard_map` is crucial. It's a pytree (often a tuple) of `PartitionSpec` objects, one for each input argument to your mapped function. Each `PartitionSpec` (aliased as `P`) tells JAX how to split the corresponding input array's dimensions across the named axes of your `Mesh`.\n",
        "\n",
        "* If an input array's dimension is mapped to a mesh axis in `P`, it's split along that array dimension by the size of that mesh axis.\n",
        "* If an input array's dimension is specified as `None` in `P`, or if a mesh axis is not mentioned at all in the `P` for a particular input, that part of the data is logically replicated for function instances that differ only along such unmentioned/`None` mesh axes. This means each relevant instance gets a full slice of the data corresponding to that dimension."
      ],
      "metadata": {
        "id": "bWcGW6hqez1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data: a 12x10 array\n",
        "input_array_in_specs = jnp.arange(12 * 10.).reshape(12, 10)\n",
        "print(f\"Original input_array_in_specs shape: {input_array_in_specs.shape}\")\n",
        "\n",
        "@partial(shard_map,\n",
        "         mesh=wizard_mesh,\n",
        "         in_specs=(P('row_of_wizards', None),), # Shard 0-axis by 'row_of_wizards', 1-axis is not split by 'col_of_wizards'\n",
        "         out_specs=P('row_of_wizards', 'col_of_wizards')) # out_specs just to make output shape clear\n",
        "def process_with_in_specs(data_block):\n",
        "  # data_block shape expected:\n",
        "  # Axis 0 of input_array_in_specs (size 12) is sharded by 'row_of_wizards' (size 4) -> 12/4 = 3.\n",
        "  # Axis 1 of input_array_in_specs (size 10) has 'None' in P, so it's not sharded by 'col_of_wizards'.\n",
        "  # So, each of the 4 'row_of_wizards' groups gets a (3, 10) block.\n",
        "  # Within each 'row_of_wizards' group, the 2 'col_of_wizards' instances get THE SAME (3,10) block.\n",
        "  return data_block # Simply return the block for observation\n",
        "\n",
        "print(\"\\nRunning process_with_in_specs:\")\n",
        "output_from_in_specs = process_with_in_specs(input_array_in_specs)\n",
        "\n",
        "# Output shape explanation:\n",
        "# Each of the 8 devices returns a (3,10) block (though blocks are replicated along 'col_of_wizards').\n",
        "# out_specs P('row_of_wizards', 'col_of_wizards') means:\n",
        "# - Concatenate the 4 blocks from 'row_of_wizards' -> 4 * 3 = 12 for the first dimension.\n",
        "# - Concatenate the 2 blocks from 'col_of_wizards' -> 2 * 10 = 20 for the second dimension.\n",
        "# Resulting shape: (12, 20)\n",
        "print(f\"\\nOutput shape from process_with_in_specs: {output_from_in_specs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQLjbPBB6zUQ",
        "outputId": "90c9cf32-c94f-4c1a-e631-9ed3634d2226"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original input_array_in_specs shape: (12, 10)\n",
            "\n",
            "Running process_with_in_specs:\n",
            "\n",
            "Output shape from process_with_in_specs: (12, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling Results: `out_specs`\n",
        "\n",
        "Similarly, `out_specs` dictates how the output blocks from each function instance are assembled into the final global array.\n",
        "\n",
        "* If a mesh axis is named in an output `PartitionSpec` for a particular output array dimension, the blocks from along that mesh axis are concatenated to form that dimension of the global output.\n",
        "* **Un-tiling**: If a mesh axis is *not* mentioned in an output `P` (or if a dimension in `P` is `None` corresponding to that mesh axis), you are promising `shard_map` that all function instances differing *only* by that unmentioned mesh axis produced the *exact same output block*. `shard_map` will then just use one of these identical blocks, effectively \"un-tiling\" the result along that mesh axis. This is common after a collective like `psum` ensures outputs are identical."
      ],
      "metadata": {
        "id": "XdSQ5ThYfTSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Full concatenation (tiling)\n",
        "@partial(shard_map, mesh=wizard_mesh, in_specs=(), out_specs=P('row_of_wizards', 'col_of_wizards'))\n",
        "def get_constant_block_tiled():\n",
        "  # Each of the 8 devices returns a 1x1 array\n",
        "  return jnp.array([[1.0]])\n",
        "\n",
        "result_tiled = get_constant_block_tiled()\n",
        "print(f\"Result with out_specs=P('row_of_wizards', 'col_of_wizards'):\\n{result_tiled}\")\n",
        "print(f\"Shape: {result_tiled.shape}\") # Expected (4*1, 2*1) = (4,2)\n",
        "\n",
        "# Example 2: Un-tiling along 'col_of_wizards'\n",
        "# This is safe because all devices return the same constant block.\n",
        "@partial(shard_map, mesh=wizard_mesh, in_specs=(), out_specs=P('row_of_wizards', None))\n",
        "def get_constant_block_untiled_col():\n",
        "  return jnp.array([[2.0]]) # Each device returns a 1x1 array\n",
        "\n",
        "result_untiled_col = get_constant_block_untiled_col()\n",
        "print(f\"\\nResult with out_specs=P('row_of_wizards', None) (un-tiling 'col_of_wizards'):\\n{result_untiled_col}\")\n",
        "print(f\"Shape: {result_untiled_col.shape}\") # Expected (4*1, 1) = (4,1)\n",
        "\n",
        "# Example 3: Un-tiling along both axes\n",
        "@partial(shard_map, mesh=wizard_mesh, in_specs=(), out_specs=P(None, None))\n",
        "def get_constant_block_untiled_both():\n",
        "  return jnp.array([[3.0]])\n",
        "\n",
        "result_untiled_both = get_constant_block_untiled_both()\n",
        "print(f\"\\nResult with out_specs=P(None, None) (un-tiling both axes):\\n{result_untiled_both}\")\n",
        "print(f\"Shape: {result_untiled_both.shape}\") # Expected (1,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlFENp30fD2h",
        "outputId": "1f4ad620-55f6-4179-976e-69bf65876f69"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result with out_specs=P('row_of_wizards', 'col_of_wizards'):\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n",
            "Shape: (4, 2)\n",
            "\n",
            "Result with out_specs=P('row_of_wizards', None) (un-tiling 'col_of_wizards'):\n",
            "[[2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]]\n",
            "Shape: (4, 1)\n",
            "\n",
            "Result with out_specs=P(None, None) (un-tiling both axes):\n",
            "[[3.]]\n",
            "Shape: (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to understand that `out_specs` primarily defines how JAX *interprets* the existing data buffers on the devices to form a logical global array. It does not cause physical data movement of output shards between devices.\n",
        "\n",
        "Conversely, `in_specs` *can* imply data movement if the input arrays passed to the `shard_map`-transformed function are not already sharded according to the `in_specs` and `Mesh`. JAX will handle reshuffling the data to match your specifications."
      ],
      "metadata": {
        "id": "znsja5wUf4a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking Data Variance: Varying Manual Axes (VMA)\n",
        "\n",
        "Inside a `shard_map`, JAX can track whether a value is the same or different across the function instances running on your mesh. This is called \"Varying Manual Axes\" (VMA) analysis. It's enabled by default with `check_vma=True`.\n",
        "\n",
        "You can inspect the VMA of an array `x` within the mapped function using `jax.typeof(x).vma`. This will show a set of mesh axis names over which `x` is currently varying.\n",
        "* If an input is sharded along a mesh axis (e.g., `'my_axis'`), its VMA will include `{'my_axis'}`.\n",
        "* If a collective operation like `jax.lax.psum` sums values across `'my_axis'`, the result of the `psum` will typically be unvarying over `'my_axis'`, so its VMA will not include `'my_axis'`."
      ],
      "metadata": {
        "id": "6_W_AbZagHgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a simple 1D mesh of 2 devices for VMA demonstration\n",
        "vma_mesh = Mesh(np.array(jax.devices()[:2]), ('batch_dim',))\n",
        "\n",
        "# Data to be sharded\n",
        "vma_data = jnp.arange(6.) # Shape (6,)\n",
        "\n",
        "@partial(shard_map, mesh=vma_mesh,\n",
        "         in_specs=P('batch_dim'), # Shard data along 'batch_dim'\n",
        "         out_specs=P())          # Expect a fully replicated (unvarying) output\n",
        "def check_vma_effects(x_block):\n",
        "  # x_block will be (3,) per device, and it varies along 'batch_dim'\n",
        "  print(f\"  Inside shard_map: x_block type: {jax.typeof(x_block)}\")\n",
        "  print(f\"  VMA of x_block: {jax.typeof(x_block).vma}\")\n",
        "\n",
        "  # Sum x_block across all devices in the 'batch_dim' mesh axis\n",
        "  y_summed = jax.lax.psum(x_block, 'batch_dim')\n",
        "  # y_summed will be the same on both devices, so it's unvarying over 'batch_dim'\n",
        "  print(f\"  Inside shard_map: y_summed type: {jax.typeof(y_summed)}\")\n",
        "  print(f\"  VMA of y_summed: {jax.typeof(y_summed).vma}\")\n",
        "\n",
        "  return y_summed\n",
        "\n",
        "print(\"Running check_vma_effects:\")\n",
        "result_vma_check = check_vma_effects(vma_data)\n",
        "print(f\"\\nFinal result_vma_check: {result_vma_check}\")\n",
        "print(f\"Final result_vma_check type: {jax.typeof(result_vma_check)}\") # VMA should be empty globally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9x1fa5fypP",
        "outputId": "09cf7ac6-5c16-499f-919d-fee5371ebe48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running check_vma_effects:\n",
            "  Inside shard_map: x_block type: ShapedArray(float32[3]{batch_dim})\n",
            "  VMA of x_block: frozenset({'batch_dim'})\n",
            "  Inside shard_map: y_summed type: ShapedArray(float32[3])\n",
            "  VMA of y_summed: frozenset()\n",
            "\n",
            "Final result_vma_check: [3. 5. 7.]\n",
            "Final result_vma_check type: ShapedArray(float32[3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `check_vma=True` setting is very useful because it allows `shard_map` to statically verify your `out_specs`. If your `out_specs` implies that an output should be replicated along a certain mesh axis (i.e., you don't mention that mesh axis in `out_specs` for concatenation), but the VMA analysis shows the output is actually varying along that axis, `shard_map` will raise an error. This helps catch subtle bugs in your sharding logic."
      ],
      "metadata": {
        "id": "lfsY3eXOikdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of check_vma catching an out_specs error\n",
        "@partial(shard_map, mesh=vma_mesh,\n",
        "         in_specs=P('batch_dim'),\n",
        "         out_specs=P()) # P() implies the output is replicated across ALL mesh axes.\n",
        "def incorrect_out_specs_example(x_block):\n",
        "  # x_block IS varying over 'batch_dim' because of in_specs=P('batch_dim').\n",
        "  # Returning it directly contradicts out_specs=P() which expects an unvarying result.\n",
        "  return x_block\n",
        "\n",
        "print(\"Attempting to run incorrect_out_specs_example (expect an error):\")\n",
        "try:\n",
        "  incorrect_out_specs_example(vma_data)\n",
        "except Exception as e:\n",
        "  print(f\"\\nSuccessfully caught an error as expected:\\n-----\\n{e}\\n-----\")\n",
        "  print(\"This error indicates that out_specs requires replication over 'batch_dim', but the output value is varying over 'batch_dim'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4336jcRiQMK",
        "outputId": "90beffaf-9517-43b4-9a06-b930d1c0443e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to run incorrect_out_specs_example (expect an error):\n",
            "\n",
            "Successfully caught an error as expected:\n",
            "-----\n",
            "shard_map applied to the function 'incorrect_out_specs_example' was given out_specs which require replication which can't be statically inferred given the mesh:\n",
            "\n",
            "The mesh given has shape (2,) with corresponding axis names ('batch_dim',).\n",
            "\n",
            "out_specs is PartitionSpec() which implies that the corresponding output value is replicated across mesh axis 'batch_dim', but could not infer replication over any axes\n",
            "\n",
            "Check if these output values are meant to be replicated over those mesh axes. If not, consider revising the corresponding out_specs entries. If so, consider disabling the check by passing the check_rep=False argument to shard_map.\n",
            "-----\n",
            "This error indicates that out_specs requires replication over 'batch_dim', but the output value is varying over 'batch_dim'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, a value might be unvarying over a mesh axis, but you need to treat it as varying, for instance, when combining it with another value that *is* varying, or when passing it as a carry in `jax.lax.scan`. For this, JAX provides `jax.lax.pvary`.\n",
        "\n",
        "`jax.lax.pvary(x, 'axis_name')` tells JAX to consider `x` as varying along `'axis_name'`, even if it was previously unvarying. It's primarily a type system hint and often a no-op at runtime, but it becomes important for correctness in automatic differentiation (where its transpose is `psum`). JAX often inserts `pvary` implicitly for binary operations to make VMA types match."
      ],
      "metadata": {
        "id": "xThPLYGTi0iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unvarying_data_for_pvary = jnp.arange(3.) # This will be replicated\n",
        "\n",
        "@partial(shard_map, mesh=vma_mesh,\n",
        "         in_specs=P(), # P() means x_unvarying is NOT sharded, so it's unvarying over 'batch_dim'\n",
        "         out_specs=P('batch_dim')) # Expect the output to be sharded/varying along 'batch_dim'\n",
        "def pvary_example_func(x_unvarying):\n",
        "  print(f\"  Inside shard_map, x_unvarying type: {jax.typeof(x_unvarying)}\")\n",
        "  print(f\"  VMA of x_unvarying: {jax.typeof(x_unvarying).vma}\")\n",
        "\n",
        "  # Now, explicitly mark it as varying over 'batch_dim'\n",
        "  y_now_varying = jax.lax.pvary(x_unvarying, 'batch_dim')\n",
        "  print(f\"  Inside shard_map, y_now_varying type: {jax.typeof(y_now_varying)}\")\n",
        "  print(f\"  VMA of y_now_varying: {jax.typeof(y_now_varying).vma}\")\n",
        "\n",
        "  # The out_specs=P('batch_dim') is consistent with y_now_varying's VMA.\n",
        "  return y_now_varying\n",
        "\n",
        "print(\"Running pvary_example_func:\")\n",
        "result_pvary = pvary_example_func(unvarying_data_for_pvary)\n",
        "print(f\"\\nFinal result_pvary: {result_pvary}\")\n",
        "print(f\"Shape of result_pvary: {result_pvary.shape}\") # Should be (2 devices * 3 elements) = (6,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QG8Mr90iuUa",
        "outputId": "da257396-1f2c-4a52-c2d5-7c0e61fe9b8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pvary_example_func:\n",
            "  Inside shard_map, x_unvarying type: ShapedArray(float32[3])\n",
            "  VMA of x_unvarying: frozenset()\n",
            "  Inside shard_map, y_now_varying type: ShapedArray(float32[3]{batch_dim})\n",
            "  VMA of y_now_varying: frozenset({'batch_dim'})\n",
            "\n",
            "Final result_pvary: [0. 1. 2. 0. 1. 2.]\n",
            "Shape of result_pvary: (6,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX automatically inserts `pvary` for simple binary operations (like `+`, `*`) if one operand is varying and the other is not, to make their VMA types match.\n",
        "\n",
        "However, for more complex operations like `jax.lax.scan`, you might need to use `jax.lax.pvary` explicitly. `jax.lax.scan` requires that the VMA type of a carry variable at the end of the loop body matches its VMA type at the beginning. If they don't match, JAX will raise an error."
      ],
      "metadata": {
        "id": "wunSk77YjdKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scan example that demonstrates a VMA type mismatch error\n",
        "scan_mesh = vma_mesh # mesh_axis_names = ('batch_dim',)\n",
        "\n",
        "# x is varying, y is unvarying\n",
        "scan_x_varying = jnp.arange(6.) # Sharded to (3,) per device\n",
        "scan_y_unvarying = jnp.arange(10., 13.) # Replicated (3,) on each device\n",
        "\n",
        "@partial(shard_map, mesh=scan_mesh,\n",
        "         in_specs=(P('batch_dim'), P()), # x is varying, y is not\n",
        "         out_specs=(P('batch_dim'), P('batch_dim')))\n",
        "def scan_vma_problem_func(x_vary_block, y_unvary_block):\n",
        "  # x_vary_block has VMA {'batch_dim'}\n",
        "  # y_unvary_block has VMA {}\n",
        "\n",
        "  def loop_body(carry, _unused_input):\n",
        "    carry_x, carry_y = carry\n",
        "    # Attempt to swap them:\n",
        "    # new_carry_x comes from carry_y (VMA {})\n",
        "    # new_carry_y comes from carry_x (VMA {'batch_dim'})\n",
        "    # This creates a VMA mismatch for scan's carry consistency rule.\n",
        "    return (carry_y, carry_x), ()\n",
        "\n",
        "  # Initial carry: (x_vary_block, y_unvary_block)\n",
        "  # Expected carry after 1st iter (if it worked): (y_unvary_block, x_vary_block)\n",
        "  # Problem:\n",
        "  # - 1st carry element: input VMA {'batch_dim'}, output VMA {} -> Mismatch!\n",
        "  # - 2nd carry element: input VMA {}, output VMA {'batch_dim'} -> Mismatch!\n",
        "  try:\n",
        "    (final_x, final_y), _ = jax.lax.scan(loop_body, (x_vary_block, y_unvary_block), (), length=2)\n",
        "    return final_x, final_y\n",
        "  except Exception as e:\n",
        "    print(f\"--- Successfully caught VMA error in scan (as expected) ---\\n{e}\\n--- End of error ---\")\n",
        "    # Return placeholders to allow the notebook to continue if run interactively\n",
        "    return jnp.array([-1.0]), jnp.array([-1.0])\n",
        "\n",
        "\n",
        "print(\"Running scan_vma_problem_func (expecting an error message):\")\n",
        "_ = scan_vma_problem_func(scan_x_varying, scan_y_unvarying)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed_sqkYbjP30",
        "outputId": "7d34247e-d80b-4468-fc79-ad5d1711c8e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running scan_vma_problem_func (expecting an error message):\n",
            "--- Successfully caught VMA error in scan (as expected) ---\n",
            "scan body function carry input and carry output must have equal types, but they differ:\n",
            "\n",
            "  * the input carry component carry[0] has type float32[3]{batch_dim} but the corresponding output carry component has type float32[3], so the varying manual axes do not match;\n",
            "  * the input carry component carry[1] has type float32[3] but the corresponding output carry component has type float32[3]{batch_dim}, so the varying manual axes do not match.\n",
            "\n",
            "This might be fixed by applying `jax.lax.pvary(..., ('batch_dim',))` to the initial carry value corresponding to the input carry component carry[1].\n",
            "See https://docs.jax.dev/en/latest/notebooks/shard_map.html#scan-vma for more information.\n",
            "\n",
            "Revise the function so that all output types match the corresponding input types.\n",
            "--- End of error ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix this, we can use `jax.lax.pvary` on the initially unvarying part of the carry (`y_unvary_block` in this case) to tell `scan` (and JAX's type system) to treat it as if it varies along the `'batch_dim'` mesh axis. This makes its VMA type consistent with `x_vary_block` for the purpose of the scan operation."
      ],
      "metadata": {
        "id": "A_QlplrKoO5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(shard_map, mesh=scan_mesh,\n",
        "         in_specs=(P('batch_dim'), P()), # x is varying, y is not initially\n",
        "         out_specs=(P('batch_dim'), P('batch_dim')))\n",
        "def scan_vma_fixed_func(x_vary_block, y_unvary_block):\n",
        "  # x_vary_block has VMA {'batch_dim'}\n",
        "  # y_unvary_block has VMA {}\n",
        "\n",
        "  # FIX: Explicitly mark y_unvary_block as varying over 'batch_dim' for scan\n",
        "  y_treated_as_varying = jax.lax.pvary(y_unvary_block, 'batch_dim')\n",
        "  # Now, y_treated_as_varying has VMA {'batch_dim'}\n",
        "\n",
        "  def loop_body(carry, _unused_input):\n",
        "    carry_x, carry_y = carry # Both now have VMA {'batch_dim'}\n",
        "    return (carry_y, carry_x), () # Swapping is fine, VMA types remain consistent\n",
        "\n",
        "  # Initial carry: (x_vary_block, y_treated_as_varying)\n",
        "  # Both elements of the carry tuple now have VMA {'batch_dim'}\n",
        "  (final_x, final_y), _ = jax.lax.scan(loop_body, (x_vary_block, y_treated_as_varying), (), length=2)\n",
        "  return final_x, final_y\n",
        "\n",
        "print(\"Running scan_vma_fixed_func:\")\n",
        "fixed_res_x, fixed_res_y = scan_vma_fixed_func(scan_x_varying, scan_y_unvarying)\n",
        "\n",
        "print(f\"\\nResult from fixed scan: final_x shape {fixed_res_x.shape}, final_y shape {fixed_res_y.shape}\")\n",
        "# After 2 iterations of swapping, final_x will hold original y values, final_y will hold original x values.\n",
        "# Example: original x_vary_block on dev0 is [0,1,2], on dev1 is [3,4,5]\n",
        "# original y_unvary_block is [10,11,12] on both.\n",
        "# y_treated_as_varying on dev0 is [10,11,12], on dev1 is [10,11,12] (but VMA is batch_dim)\n",
        "# Iter 1: carry_x=y_t_a_v, carry_y=x_v_b\n",
        "# Iter 2: carry_x=x_v_b, carry_y=y_t_a_v -> so final_x is x_v_b, final_y is y_t_a_v\n",
        "# This means fixed_res_x should be like scan_x_varying, and fixed_res_y like scan_y_unvarying tiled.\n",
        "print(f\"fixed_res_x (should correspond to original x values):\\n{fixed_res_x}\")\n",
        "print(f\"fixed_res_y (should correspond to original y values, now sharded):\\n{fixed_res_y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZQ1Nn6NkAYz",
        "outputId": "ec0934a1-6d9d-4587-8546-d27df299ba0b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running scan_vma_fixed_func:\n",
            "\n",
            "Result from fixed scan: final_x shape (6,), final_y shape (6,)\n",
            "fixed_res_x (should correspond to original x values):\n",
            "[0. 1. 2. 3. 4. 5.]\n",
            "fixed_res_y (should correspond to original y values, now sharded):\n",
            "[10. 11. 12. 10. 11. 12.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen how VMA helps ensure correctness. Now, let's explore how different function instances in `shard_map` actually communicate and coordinate. This is done using **Collective Operations**. These are special functions within `jax.lax` that operate across devices in a mesh, using the mesh axis names you've defined."
      ],
      "metadata": {
        "id": "jo0NEo0FpJCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collective Operation: `jax.lax.psum` (Parallel Sum)**\n",
        "\n",
        "The first collective we'll look at (and which we've already used) is `jax.lax.psum(x, axis_name)`.\n",
        "This computes an all-reduce sum of `x` across all devices along the specified `axis_name` (or a tuple of axis names).\n",
        "* Each device contributes its local value of `x`.\n",
        "* All devices participating in the sum receive the *same* total sum.\n",
        "* Because the result is identical on all devices along the `axis_name` used for the sum, this often allows you to use an un-tiled `out_spec` for that dimension (e.g., `P(None)` if `axis_name` was the only mesh axis, or `P('other_axis', None)` if summing along one axis of a 2D mesh)."
      ],
      "metadata": {
        "id": "thJjHgknpe3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# psum example with a 1D mesh\n",
        "# Let's use 4 devices for this\n",
        "psum_mesh_1d = Mesh(np.array(jax.devices()[:4]), ('device_axis',))\n",
        "\n",
        "# Each device will have a small array\n",
        "data_for_psum = jnp.arange(4. * 2.).reshape(4, 2) # Global data: 4 devices, each gets a (1,2) block\n",
        "\n",
        "@partial(shard_map, mesh=psum_mesh_1d,\n",
        "         in_specs=P('device_axis'),    # Shard along 'device_axis', each device gets a (1,2) block\n",
        "         out_specs=P(None))          # Output is replicated (un-tiled)\n",
        "def psum_example_1d(x_block):\n",
        "  # x_block is (1,2) on each of the 4 devices\n",
        "  # Sum x_block across all devices along 'device_axis'\n",
        "  summed_block = jax.lax.psum(x_block, 'device_axis')\n",
        "  # summed_block will be identical on all 4 devices.\n",
        "  # Its VMA over 'device_axis' will be empty.\n",
        "  print(f\"AFTER psum VMA: {jax.typeof(summed_block).vma}\")\n",
        "\n",
        "  return summed_block\n",
        "\n",
        "print(\"Running psum_example_1d:\")\n",
        "result_psum_1d = psum_example_1d(data_for_psum)\n",
        "\n",
        "print(f\"\\nFinal result_psum_1d: {result_psum_1d}\")\n",
        "print(f\"Shape of final result: {result_psum_1d.shape}\") # Expected (1,2) because of P(None) out_spec\n",
        "\n",
        "# Manually calculate what we expect:\n",
        "# Device 0 block: [[0., 1.]]\n",
        "# Device 1 block: [[2., 3.]]\n",
        "# Device 2 block: [[4., 5.]]\n",
        "# Device 3 block: [[6., 7.]]\n",
        "# Sum: [[0+2+4+6, 1+3+5+7]] = [[12., 16.]]\n",
        "expected_sum = jnp.sum(data_for_psum.reshape(4,1,2), axis=0) # Reshape to be (num_devices, block_shape)\n",
        "print(f\"Expected sum (manual calculation): {expected_sum}\")\n",
        "assert allclose(result_psum_1d, expected_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKRulmWqp8sr",
        "outputId": "156f7af1-f9fe-409b-e495-dcb7b570edf9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running psum_example_1d:\n",
            "AFTER psum VMA: frozenset()\n",
            "\n",
            "Final result_psum_1d: [[12. 16.]]\n",
            "Shape of final result: (1, 2)\n",
            "Expected sum (manual calculation): [[12. 16.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above, `out_specs=P(None)` was used. Since `psum` over `'device_axis'` ensures every device in that mesh axis gets the same summed result, we don't need to concatenate these identical results. `P(None)` tells `shard_map` to just pick one copy. If it were `P('device_axis')`, the (1,2) result from each of the 4 devices would be concatenated, yielding a (4,2) array where each row is identical.\n",
        "\n",
        "`psum` can also operate over multiple mesh axes if you have a multi-dimensional mesh."
      ],
      "metadata": {
        "id": "oc8IxOpH11Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collective Operation: `jax.lax.all_gather`\n",
        "\n",
        "Another fundamental collective is `jax.lax.all_gather(x, axis_name, tiled=...)`. This operation gathers the local arrays `x` from all devices along the specified `axis_name` and makes the combined data available to each of those devices.\n",
        "\n",
        "* `axis_name`: The mesh axis (or axes) along which to gather.\n",
        "* `tiled=True`: The gathered blocks are concatenated along an existing axis of `x`. The size of this axis in the output will be the original size multiplied by the number of devices in `axis_name`.\n",
        "* `tiled=False` (default): The gathered blocks are stacked along a *new* axis, effectively increasing the rank of the array. The size of this new axis will be the number of devices in `axis_name`."
      ],
      "metadata": {
        "id": "Hv5XiYMX2H25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_gather example with a 1D mesh and tiled=True\n",
        "# Let's use 4 devices again\n",
        "all_gather_mesh_1d = Mesh(np.array(jax.devices()[:4]), ('gather_axis',))\n",
        "\n",
        "# Each device has a small, unique array, e.g., just a scalar for simplicity\n",
        "data_for_all_gather = jnp.arange(4, dtype=jnp.float32) # Global data: [0., 1., 2., 3.]\n",
        "# With in_specs=P('gather_axis'), device 0 gets [0.], device 1 gets [1.], etc.\n",
        "\n",
        "def all_gather_example_tiled(x_block):\n",
        "  # x_block is a scalar (shape ()) on each device\n",
        "  print(f\"  BEFORE all_gather: x_block = {x_block} (shape {x_block.shape})\")\n",
        "\n",
        "  # Gather x_block from all devices along 'gather_axis'\n",
        "  # tiled=True means concatenate. Since x_block is scalar, effectively it creates a 1D array.\n",
        "  # If x_block was, say, (2,), and gather_axis_size=4, result would be (2*4=8,).\n",
        "  # Here, x_block=(), result on each device will be (4,).\n",
        "  gathered_data = jax.lax.all_gather(x_block, 'gather_axis', tiled=True)\n",
        "\n",
        "  print(f\"  AFTER all_gather: gathered_data = {gathered_data} (shape {gathered_data.shape})\")\n",
        "  # Each device now has the full array [0., 1., 2., 3.]\n",
        "\n",
        "  # For the out_specs P('gather_axis') to make sense with a (4,) array on each device,\n",
        "  # the output needs to be sharded. So, let's return a processed piece.\n",
        "  # Or, if we wanted each device to have the full gathered array, out_specs would be P().\n",
        "  # Let's assume we process this gathered_data and the output is meant to be sharded again.\n",
        "  # For this example, let's just show the gathered data for one device and adjust out_specs.\n",
        "  return gathered_data\n",
        "\n",
        "print(\"Running all_gather_example_tiled (tiled=True):\")\n",
        "result_all_gather_tiled = shard_map(\n",
        "    all_gather_example_tiled,\n",
        "    mesh=all_gather_mesh_1d,\n",
        "    in_specs=P('gather_axis'), # Each device gets one scalar from data_for_all_gather\n",
        "    out_specs=P('gather_axis'), # Output will be the concatenated result, sharded\n",
        ")(data_for_all_gather)\n",
        "\n",
        "print(f\"\\nFinal result_all_gather_tiled: {result_all_gather_tiled}\")\n",
        "print(f\"Shape of final result: {result_all_gather_tiled.shape}\") # Expected (4,)\n",
        "\n",
        "\n",
        "# Modify out_specs for clarity: If each device has the full gathered array,\n",
        "# and we want the global output to be that full array (replicated), use P().\n",
        "print(\"Running all_gather_example_tiled_replicated_out (tiled=True):\")\n",
        "result_all_gather_tiled = shard_map(\n",
        "    all_gather_example_tiled,\n",
        "    mesh=all_gather_mesh_1d,\n",
        "    in_specs=P('gather_axis'),\n",
        "    out_specs=P(), # Output is the full gathered array, replicated.\n",
        "    check_rep=False, # would be check_vma after update, We need jax.lax.all_gather_invariant without this. (no API yet)\n",
        ")(data_for_all_gather)\n",
        "print(f\"\\nFinal result_all_gather_tiled: {result_all_gather_tiled}\")\n",
        "print(f\"Shape of final result: {result_all_gather_tiled.shape}\") # Expected (4,)\n",
        "\n",
        "# All devices should have [0., 1., 2., 3.] after all_gather.\n",
        "# With out_specs=P(), the global result is this replicated array.\n",
        "assert allclose(result_all_gather_tiled, data_for_all_gather)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BTmwG5nyMEw",
        "outputId": "9d64692e-0d15-4588-d3a1-0b1cd2386029"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running all_gather_example_tiled (tiled=True):\n",
            "  BEFORE all_gather: x_block = On TFRT_CPU_0 at mesh coordinates (gather_axis,) = (0,):\n",
            "[0.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (gather_axis,) = (1,):\n",
            "[1.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (gather_axis,) = (2,):\n",
            "[2.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (gather_axis,) = (3,):\n",
            "[3.]\n",
            " (shape (1,))\n",
            "  AFTER all_gather: gathered_data = On TFRT_CPU_0 at mesh coordinates (gather_axis,) = (0,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (gather_axis,) = (1,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (gather_axis,) = (2,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (gather_axis,) = (3,):\n",
            "[0. 1. 2. 3.]\n",
            " (shape (4,))\n",
            "\n",
            "Final result_all_gather_tiled: [0. 1. 2. 3. 0. 1. 2. 3. 0. 1. 2. 3. 0. 1. 2. 3.]\n",
            "Shape of final result: (16,)\n",
            "Running all_gather_example_tiled_replicated_out (tiled=True):\n",
            "  BEFORE all_gather: x_block = On TFRT_CPU_0 at mesh coordinates (gather_axis,) = (0,):\n",
            "[0.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (gather_axis,) = (1,):\n",
            "[1.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (gather_axis,) = (2,):\n",
            "[2.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (gather_axis,) = (3,):\n",
            "[3.]\n",
            " (shape (1,))\n",
            "  AFTER all_gather: gathered_data = On TFRT_CPU_0 at mesh coordinates (gather_axis,) = (0,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (gather_axis,) = (1,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (gather_axis,) = (2,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (gather_axis,) = (3,):\n",
            "[0. 1. 2. 3.]\n",
            " (shape (4,))\n",
            "\n",
            "Final result_all_gather_tiled: [0. 1. 2. 3.]\n",
            "Shape of final result: (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VMA and `out_specs` for `all_gather`:**\n",
        "\n",
        "The output of `jax.lax.all_gather` is generally considered **varying** over the `axis_name` it gathered along. This is due to how its gradient (which involves `psum_scatter`) is handled in JAX's automatic differentiation system.\n",
        "\n",
        "Therefore, if you use `check_vma=True` (the default):\n",
        "* If you return the direct result of `all_gather` and your `out_specs` implies replication along the gathered axis (e.g., `P()` when gathering along the only mesh axis), you might encounter a VMA error.\n",
        "* A common pattern is for `out_specs` to be `P('gather_axis')`, meaning the final global array is formed by concatenating these (now identical, but still typed as varying) gathered arrays from each device. This results in a larger array where the gathered data is repeated.\n",
        "* If you truly need an output that is typed as invariant, and you are sure the `all_gather` produces identical results (which it does by definition), you might consider `jax.lax.all_gather_invariant` (though its user API is not present yet) or simply structure your computation so the `all_gather` is an intermediate step and a subsequent operation (like a `psum` or using the data in a replicated way) makes the final returned value invariant.\n",
        "\n",
        "For simplicity in examples, we often use `out_specs=P()` after an `all_gather` if the goal is just to demonstrate that each device now possesses the full data. In more complex scenarios, careful consideration of VMA and `out_specs` is needed."
      ],
      "metadata": {
        "id": "LxVhudSz7sdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(shard_map, mesh=all_gather_mesh_1d,\n",
        "         in_specs=P('gather_axis'),   # Each device gets one scalar\n",
        "         out_specs=P(),\n",
        "         check_rep=False)               # Output is the full gathered array, replicated\n",
        "def all_gather_example_stacked(x_block):\n",
        "  # x_block is a scalar on each device\n",
        "\n",
        "  # Gather x_block from all devices along 'gather_axis'\n",
        "  # tiled=False means stack along a new axis (default axis=0 for the new dim).\n",
        "  # Input x_block is shape (). Mesh size along 'gather_axis' is 4.\n",
        "  # Result on each device will be shape (4,1).\n",
        "  gathered_data_stacked = jax.lax.all_gather(x_block, 'gather_axis', tiled=False)\n",
        "  print(f\"  AFTER all_gather: gathered_data = {gathered_data_stacked} (shape {gathered_data_stacked.shape})\")\n",
        "  # print(f\"  Device {device_idx} (stacked) AFTER all_gather: gathered_data_stacked = {gathered_data_stacked}\")\n",
        "  # Each device now has [[0.], [1.], [2.], [3.]] if x_block was scalar,\n",
        "  # or more generally, if x_block was shape (S,), result is (4, S).\n",
        "  # Since x_block is scalar here, shape is (4,1).\n",
        "\n",
        "  return gathered_data_stacked\n",
        "\n",
        "print(\"Running all_gather_example_stacked (tiled=False):\")\n",
        "result_all_gather_stacked = all_gather_example_stacked(data_for_all_gather)\n",
        "\n",
        "print(f\"\\nFinal result_all_gather_stacked: {result_all_gather_stacked}\")\n",
        "print(f\"Shape of final result: {result_all_gather_stacked.shape}\") # Expected (4,) because scalars stack into a 1D array.\n",
        "\n",
        "print(f\"Shape {data_for_all_gather.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn0XU2V93BGq",
        "outputId": "0a82996e-9c60-4209-ecae-3457921ec5e3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running all_gather_example_stacked (tiled=False):\n",
            "  AFTER all_gather: gathered_data = On TFRT_CPU_0 at mesh coordinates (gather_axis,) = (0,):\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (gather_axis,) = (1,):\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (gather_axis,) = (2,):\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (gather_axis,) = (3,):\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            " (shape (4, 1))\n",
            "\n",
            "Final result_all_gather_stacked: [[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "Shape of final result: (4, 1)\n",
            "Shape data_for_all_gather.shape=(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`jax.lax.all_gather` is particularly useful in scenarios like:\n",
        "* **Fully Sharded Data Parallelism (FSDP):** When model parameters are sharded across devices, but a full parameter tensor is needed for a local computation (e.g., a matrix multiplication). The sharded parameter can be all-gathered before the operation.\n",
        "* When an intermediate activation is sharded, but the next layer requires access to the full activation on each device.\n",
        "It allows each device to obtain a complete copy of data that was previously distributed across a mesh axis."
      ],
      "metadata": {
        "id": "uF6zdQGv_Ka2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collective Operation: `jax.lax.psum_scatter`\n",
        "\n",
        "Next is `jax.lax.psum_scatter(x, axis_name, scatter_dimension=0, tiled=False)`.\n",
        "This collective is like `psum` in that it computes a sum of `x` across devices along `axis_name`. However, instead of every device receiving the *full* sum, each device receives only a *shard* of that total sum. The `scatter_dimension` argument specifies which axis of `x` (the local array) is used for scattering the summed result.\n",
        "\n",
        "* `tiled=False` (default): The `scatter_dimension` of the input `x` must match the size of the `axis_name` (or the relevant subgroup of devices). This dimension is effectively consumed by the scatter, and the output on each device will have one less rank than the input `x`. Each device `i` along `axis_name` gets the `i`-th slice of the sum.\n",
        "* `tiled=True`: The `scatter_dimension` of input `x` must be divisible by the size of `axis_name`. The output on each device will have the same rank as `x`, but the `scatter_dimension` will be divided by the size of `axis_name`. This is often more intuitive when you think of the input `x` itself as already being a \"shard\" of a larger conceptual array that you want to sum and then re-shard."
      ],
      "metadata": {
        "id": "Dp-UoDRt_8T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# psum_scatter example with a 1D mesh and tiled=True\n",
        "# Let's use 4 devices\n",
        "psum_scatter_mesh_1d = Mesh(np.array(jax.devices()[:4]), ('scatter_axis',))\n",
        "\n",
        "# Global data: an array of shape (4, 4).\n",
        "# Each of the 4 devices will receive a (1, 4) row if in_specs is P('scatter_axis').\n",
        "# For psum_scatter to scatter the result, the input x_block itself is often the full piece that will be summed.\n",
        "# Let's define an input that is already sharded. Each device has a vector of 4 elements.\n",
        "# The sum of these vectors will be a vector of 4 elements.\n",
        "# Each device will get 1 element of this summed vector.\n",
        "\n",
        "# Input: Each device has a vector [d, d, d, d] where d is device index.\n",
        "# Device 0: [0,0,0,0], Device 1: [1,1,1,1], Device 2: [2,2,2,2], Device 3: [3,3,3,3]\n",
        "# Sum across devices: [0+1+2+3, 0+1+2+3, 0+1+2+3, 0+1+2+3] = [6,6,6,6]\n",
        "# psum_scatter (tiled=True, scatter_dimension=0) will give:\n",
        "# Device 0 gets [6] (the first element of the sum, because its input axis 0 was size 4, output axis 0 is 4/4=1)\n",
        "# Device 1 gets [6]\n",
        "# Device 2 gets [6]\n",
        "# Device 3 gets [6]\n",
        "# This example needs careful setup of input data for tiled=True to make sense for scattering.\n",
        "\n",
        "# Each device has a (1, 4) block. The sum over devices results in a (1, 4) array.\n",
        "# This (1, 4) sum is then scattered. If scatter_dimension=1, and tiled=True,\n",
        "# the output on each device is (1, 4 / num_devices).\n",
        "data_for_pscatter = jnp.arange(16, dtype=jnp.float32).reshape(4, 4)\n",
        "# if in_specs=P('scatter_axis'), device 0 gets [[0,1,2,3]], device 1 gets [[4,5,6,7]] etc.\n",
        "\n",
        "@partial(shard_map, mesh=psum_scatter_mesh_1d,\n",
        "         in_specs=P('scatter_axis'),    # Each device gets a (1,4) row from data_for_pscatter\n",
        "         out_specs=P('scatter_axis')) # Concatenate the scattered (1,1) results from each device\n",
        "def psum_scatter_example_tiled(x_block):\n",
        "  # x_block is (4,) on each of the 4 devices.\n",
        "  print(f\"  BEFORE psum_scatter: x_block = {x_block.shape}\")\n",
        "\n",
        "  # Sum x_block across 'scatter_axis'. The total sum will be a (1,4) vector.\n",
        "  # E.g., for first element: 0+4+8+12 = 24. Total sum = [24, 28, 32, 36].\n",
        "  # Scatter this sum. With tiled=True and scatter_dimension=1,\n",
        "  # each device gets a slice of size 4/4=1 from this sum.\n",
        "  # Device 0 gets [24], Device 1 gets [28], etc.\n",
        "  scattered_sum_block = jax.lax.psum_scatter(x_block, 'scatter_axis', scatter_dimension=1, tiled=True)\n",
        "\n",
        "  print(f\"  AFTER psum_scatter: scattered_sum_block = {scattered_sum_block.shape}\")\n",
        "  return scattered_sum_block\n",
        "\n",
        "print(\"Running psum_scatter_example_tiled (tiled=True):\")\n",
        "result_pscatter_tiled = psum_scatter_example_tiled(data_for_pscatter)\n",
        "\n",
        "print(f\"\\nFinal result_pscatter_tiled: {result_pscatter_tiled}\")\n",
        "# Each device produces a (1,1) block. out_specs=P('scatter_axis') concatenates them.\n",
        "# So, the global output is (4,1).\n",
        "print(f\"Shape of final result: {result_pscatter_tiled.shape}\")\n",
        "\n",
        "# Manually calculate expected result\n",
        "# Device 0 input: [[0,1,2,3]]\n",
        "# Device 1 input: [[4,5,6,7]]\n",
        "# Device 2 input: [[8,9,10,11]]\n",
        "# Device 3 input: [[12,13,14,15]]\n",
        "# Element-wise sum: [[0+4+8+12], [1+5+9+13], [2+6+10+14], [3+7+11+15]] = [[24], [28], [32], [36]]\n",
        "# scattered_sum_block on device 0: [[24]] (first element of total sum because scatter_dimension=0)\n",
        "# scattered_sum_block on device 1: [[28]] (second element of total sum)\n",
        "# ... this interpretation is subtle with tiled=True if scatter_dimension is not what's being sharded for output.\n",
        "# The doc says: psum_scatter(x, 'i', tiled=True) -> output on dev_i is sum(x_j)[i] (if sum is reshaped)\n",
        "# More precisely for tiled=True: output_shape[scatter_dim] = input_shape[scatter_dim] / num_devices_in_axis\n",
        "# And device k gets the k-th slice of size input_shape[scatter_dim] / num_devices_in_axis of the total sum.\n",
        "# So, if total sum is S (shape (4,)), output on device k is S[k:(k+1)].\n",
        "expected_pscatter_result = jnp.array([24., 28., 32., 36.], dtype=jnp.float32)\n",
        "assert allclose(result_pscatter_tiled.flatten(), expected_pscatter_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr26zqpB9tLD",
        "outputId": "89c4b686-b3df-4f26-85c4-fe3fab7d3c52"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running psum_scatter_example_tiled (tiled=True):\n",
            "  BEFORE psum_scatter: x_block = (1, 4)\n",
            "  AFTER psum_scatter: scattered_sum_block = (1,)\n",
            "\n",
            "Final result_pscatter_tiled: [24. 28. 32. 36.]\n",
            "Shape of final result: (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between `psum`, `psum_scatter`, and `all_gather`**\n",
        "\n",
        "Interestingly, a full `psum` (where every device gets the total sum) can be thought of as performing a `psum_scatter` (where each device gets a piece of the sum) followed by an `all_gather` (where each device collects all the pieces to reconstruct the full sum).\n",
        "\n",
        "`psum(x, axis) == all_gather(psum_scatter(x, axis, tiled=False, scatter_dimension=0), axis, tiled=True)`\n",
        "\n",
        "(The `scatter_dimension` and `tiled` arguments need to be set appropriately for the shapes to align). This decomposition is often how `psum` is implemented efficiently in practice, especially on TPUs. `psum_scatter` does about half the communication of a full `psum`."
      ],
      "metadata": {
        "id": "Kc7CJYfuDeuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(shard_map, mesh=psum_scatter_mesh_1d,\n",
        "         in_specs=P('scatter_axis'),\n",
        "         out_specs=P(),\n",
        "         check_rep=False) # For psum, the output is replicated\n",
        "def psum_via_scatter_gather(x_block):\n",
        "  # x_block is (4,) on each device\n",
        "\n",
        "  # 1. First half: Reduce-Scatter. Each device gets a part of the sum.\n",
        "  # Total sum is [24, 28, 32, 36].\n",
        "  # scattered_piece on dev 0 is [24.], dev 1 is [28.], etc. Shape (1,) on each device.\n",
        "  scattered_piece = jax.lax.psum_scatter(x_block, 'scatter_axis', scatter_dimension=1, tiled=False)\n",
        "\n",
        "  # 2. Second half: All-Gather. Each device gathers all scattered pieces.\n",
        "  # Input to all_gather is (1,) on each device.\n",
        "  # Output will be (4,) on each device, containing [24., 28., 32., 36.].\n",
        "  full_sum_replicated = jax.lax.all_gather(scattered_piece, 'scatter_axis', tiled=True)\n",
        "\n",
        "  return full_sum_replicated\n",
        "\n",
        "print(\"Running psum_via_scatter_gather:\")\n",
        "result_composed_psum = psum_via_scatter_gather(data_for_pscatter)\n",
        "print(f\"Result from psum_via_scatter_gather: {result_composed_psum}\")\n",
        "print(f\"Shape: {result_composed_psum.shape}\")\n",
        "\n",
        "\n",
        "# Compare with direct psum\n",
        "@partial(shard_map, mesh=psum_scatter_mesh_1d,\n",
        "         in_specs=P('scatter_axis'),\n",
        "         out_specs=P())\n",
        "def direct_psum_func(x_block):\n",
        "    return jax.lax.psum(x_block, 'scatter_axis')\n",
        "\n",
        "result_direct_psum = direct_psum_func(data_for_pscatter)\n",
        "print(f\"\\nResult from direct_psum_func: {result_direct_psum}\")\n",
        "print(f\"Shape: {result_direct_psum.shape}\")\n",
        "\n",
        "print(\"\\nResults match: psum is equivalent to psum_scatter + all_gather.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaeBTiknCNAP",
        "outputId": "09f128ef-5cd7-47fa-e876-e690babc15b5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running psum_via_scatter_gather:\n",
            "Result from psum_via_scatter_gather: [24. 28. 32. 36.]\n",
            "Shape: (4,)\n",
            "\n",
            "Result from direct_psum_func: [[24. 28. 32. 36.]]\n",
            "Shape: (1, 4)\n",
            "\n",
            "Results match: psum is equivalent to psum_scatter + all_gather.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collective Operation: `jax.lax.ppermute` (Parallel Permute)\n",
        "\n",
        "The `jax.lax.ppermute(x, axis_name, perm=...)` collective allows for direct data exchange between devices (function instances) along a specified `axis_name`.\n",
        "* `x`: The local array on each device that will be sent.\n",
        "* `axis_name`: The mesh axis along which the permutation occurs.\n",
        "* `perm`: A list of `(source_device_index, destination_device_index)` pairs. This defines which device sends its `x` to which other device. The indices are relative to the `axis_name`.\n",
        "\n",
        "For example, if device `s` is a source and device `d` is its destination in a pair `(s, d)`, then device `d` will receive the `x` value that was originally on device `s`."
      ],
      "metadata": {
        "id": "yeNccGziIn5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ppermute example: cyclic shift on a 1D mesh\n",
        "# Using 4 devices\n",
        "ppermute_mesh_1d = Mesh(np.array(jax.devices()[:4]), ('shift_axis',))\n",
        "num_devices_in_shift_axis = ppermute_mesh_1d.shape['shift_axis']\n",
        "\n",
        "# Each device has its index as its data (as a scalar array)\n",
        "data_for_ppermute = jnp.arange(num_devices_in_shift_axis, dtype=jnp.float32)\n",
        "# With in_specs=P('shift_axis'), device i gets jnp.array(float(i))\n",
        "\n",
        "# Define the permutation: device i sends to (i+1) % num_devices\n",
        "cyclic_perm = [(i, (i + 1) % num_devices_in_shift_axis) for i in range(num_devices_in_shift_axis)]\n",
        "print(f\"Cyclic permutation defined: {cyclic_perm}\")\n",
        "\n",
        "@partial(shard_map, mesh=ppermute_mesh_1d,\n",
        "         in_specs=P('shift_axis'),\n",
        "         out_specs=P('shift_axis'))\n",
        "def ppermute_cyclic_shift(x_block):\n",
        "  print(f\"  BEFORE ppermute: x_block = {x_block}\")\n",
        "\n",
        "  permuted_block = jax.lax.ppermute(x_block, axis_name='shift_axis', perm=cyclic_perm)\n",
        "\n",
        "  print(f\"  AFTER ppermute:  permuted_block = {permuted_block}\")\n",
        "  return permuted_block\n",
        "\n",
        "print(\"\\nRunning ppermute_cyclic_shift:\")\n",
        "result_ppermute = ppermute_cyclic_shift(data_for_ppermute)\n",
        "\n",
        "print(f\"\\nFinal result_ppermute: {result_ppermute}\")\n",
        "# Expected: Device 0 had 0, sent to 1. Device 1 had 1, sent to 2. ... Device 3 had 3, sent to 0.\n",
        "# So, device 0 receives 3. Device 1 receives 0. Device 2 receives 1. Device 3 receives 2.\n",
        "# Global result (concatenated by out_specs=P('shift_axis')): [3., 0., 1., 2.]\n",
        "expected_ppermute_result = jnp.array([3., 0., 1., 2.], dtype=jnp.float32)\n",
        "assert allclose(result_ppermute, expected_ppermute_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNYpUiUKFIcy",
        "outputId": "909f0b7f-d0fc-4808-a0c6-39a8760724fc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyclic permutation defined: [(0, 1), (1, 2), (2, 3), (3, 0)]\n",
            "\n",
            "Running ppermute_cyclic_shift:\n",
            "  BEFORE ppermute: x_block = On TFRT_CPU_0 at mesh coordinates (shift_axis,) = (0,):\n",
            "[0.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (shift_axis,) = (1,):\n",
            "[1.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (shift_axis,) = (2,):\n",
            "[2.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (shift_axis,) = (3,):\n",
            "[3.]\n",
            "\n",
            "  AFTER ppermute:  permuted_block = On TFRT_CPU_0 at mesh coordinates (shift_axis,) = (0,):\n",
            "[3.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (shift_axis,) = (1,):\n",
            "[0.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (shift_axis,) = (2,):\n",
            "[1.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (shift_axis,) = (3,):\n",
            "[2.]\n",
            "\n",
            "\n",
            "Final result_ppermute: [3. 0. 1. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes on `ppermute`:**\n",
        "* **Uniqueness:** In the `perm` list, each device index should appear at most once as a source and at most once as a destination.\n",
        "* **Unspecified Destinations:** If a device index does not appear as a `destination_index` in any pair in `perm`, the corresponding device instance will receive an array of zeros of the appropriate shape and dtype.\n",
        "* `ppermute` is a fundamental building block for many complex communication patterns."
      ],
      "metadata": {
        "id": "U7a6BGEpJNyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Other Collectives with `ppermute`**\n",
        "\n",
        "More complex collectives can often be constructed using `ppermute`. For instance, `psum_scatter` (and by extension, `psum`) can be implemented using a sequence of `ppermute` operations that pass data between neighboring devices in a ring-like fashion, with local additions at each step.\n",
        "\n",
        "Imagine a \"bucket brigade\" or a ring reduction:\n",
        "1. Each device starts with its local value (or a portion of it if the data is further broken down).\n",
        "2. In each step `k`:\n",
        "   a. Devices send their current accumulated value to their neighbor (e.g., device `i` sends to `(i-1) % N`).\n",
        "   b. Devices receive a value from their other neighbor and add it to their next piece of local data (or their ongoing accumulation).\n",
        "3. After `N-1` steps (for `N` devices), each device `i` can end up with the `i`-th component of the total scattered sum.\n",
        "\n",
        "This illustrates how point-to-point communication (`ppermute`) can build up global reductions."
      ],
      "metadata": {
        "id": "burORijuJZbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collective Operation: `jax.lax.all_to_all`\n",
        "\n",
        "The `jax.lax.all_to_all(x, axis_name, split_axis, concat_axis, tiled=...)` collective is used for more complex data redistributions. It essentially performs a \"block transpose\" of data across devices and within the local data arrays.\n",
        "\n",
        "Here's how it works conceptually:\n",
        "1. Each local array `x` on a device is split into blocks along its `split_axis` (a regular data dimension). The number of blocks is equal to the size of the mesh `axis_name`.\n",
        "2. These blocks are then exchanged between devices: device `i` sends its `j`-th block to device `j`, and device `j` receives the `i`-th block from device `i` (this is a conceptual simplification; the actual exchange pattern ensures all data is redistributed).\n",
        "3. On each receiving device, the collected blocks are concatenated along the `concat_axis` (another regular data dimension) to form the output array for that device."
      ],
      "metadata": {
        "id": "lhp1VhVcNGHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_to_all example with a 1D mesh and tiled=True\n",
        "all_to_all_mesh = Mesh(np.array(jax.devices()[:4]), ('device_group',))\n",
        "mesh_size = all_to_all_mesh.size\n",
        "\n",
        "data_for_all_to_all = jnp.arange(4. * 4.).astype(jnp.float32)\n",
        "\n",
        "@partial(shard_map, mesh=all_to_all_mesh,\n",
        "         in_specs=P('device_group'),\n",
        "         out_specs=P('device_group'))\n",
        "def all_to_all_example(x_block):\n",
        "  print(f\"  BEFORE all_to_all: x_block = \\n{x_block}\")\n",
        "  y_block = jax.lax.all_to_all(x_block, 'device_group', split_axis=0, concat_axis=0, tiled=True)\n",
        "  print(f\"  AFTER all_to_all:  y_block = \\n{y_block}\")\n",
        "  return y_block\n",
        "\n",
        "print(\"Running all_to_all_example (tiled=True):\")\n",
        "result_all_to_all = all_to_all_example(data_for_all_to_all)\n",
        "\n",
        "print(f\"\\nFinal global result_all_to_all:\\n{result_all_to_all}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXcd6HHJLXMO",
        "outputId": "6a6a0c46-f07f-4f81-c780-78a6334ff1b5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running all_to_all_example (tiled=True):\n",
            "  BEFORE all_to_all: x_block = \n",
            "On TFRT_CPU_0 at mesh coordinates (device_group,) = (0,):\n",
            "[0. 1. 2. 3.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (device_group,) = (1,):\n",
            "[4. 5. 6. 7.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (device_group,) = (2,):\n",
            "[ 8.  9. 10. 11.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (device_group,) = (3,):\n",
            "[12. 13. 14. 15.]\n",
            "\n",
            "  AFTER all_to_all:  y_block = \n",
            "On TFRT_CPU_0 at mesh coordinates (device_group,) = (0,):\n",
            "[ 0.  4.  8. 12.]\n",
            "\n",
            "On TFRT_CPU_1 at mesh coordinates (device_group,) = (1,):\n",
            "[ 1.  5.  9. 13.]\n",
            "\n",
            "On TFRT_CPU_2 at mesh coordinates (device_group,) = (2,):\n",
            "[ 2.  6. 10. 14.]\n",
            "\n",
            "On TFRT_CPU_3 at mesh coordinates (device_group,) = (3,):\n",
            "[ 3.  7. 11. 15.]\n",
            "\n",
            "\n",
            "Final global result_all_to_all:\n",
            "[ 0.  4.  8. 12.  1.  5.  9. 13.  2.  6. 10. 14.  3.  7. 11. 15.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The `tiled` argument in `all_to_all`:**\n",
        "\n",
        "* **`tiled=True`**: This is often the more intuitive mode for block transposes.\n",
        "    * The size of the `split_axis` in the local input `x` must be evenly divisible by the size of the mesh `axis_name`.\n",
        "    * The output local array on each device will have the same rank and shape as the input local array `x`, but with data permuted. The `split_axis` is divided into chunks, distributed, and then these chunks are concatenated back along `concat_axis`. If `split_axis == concat_axis`, the dimension size effectively remains the same but is now composed of data from different original devices.\n",
        "\n",
        "* **`tiled=False`** (default):\n",
        "    * The size of the `split_axis` in the local input `x` must be equal to the size of the mesh `axis_name`.\n",
        "    * A *new* axis (whose size is the mesh `axis_name` size) is created at the position specified by `concat_axis` in the output. The original `split_axis` is removed. This changes the rank of the local array.\n",
        "\n",
        "For many common use cases like transposing blocks of data, `tiled=True` is used."
      ],
      "metadata": {
        "id": "vhTry5W4P16G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: The Power of `shard_map`\n",
        "\n",
        "This tutorial has introduced you to `jax.shard_map`, a powerful tool for explicit, manual control over multi-device parallelism in JAX. We've covered:\n",
        "* The concept of a `Mesh` and `PartitionSpec` for defining data and computation layout.\n",
        "* How `in_specs` and `out_specs` control data sharding and assembly.\n",
        "* The rank-preserving nature of `shard_map` compared to `vmap`.\n",
        "* Varying Manual Axes (VMA) for type checking and `jax.lax.pvary` for managing variance.\n",
        "* Fundamental collective operations: `psum`, `all_gather`, `psum_scatter`, `ppermute`, and `all_to_all`.\n",
        "\n",
        "`shard_map` allows you to implement sophisticated parallel algorithms by giving you direct control over how data is split, where computation happens, and how devices communicate. It composes seamlessly with `jax.jit` for compilation and `jax.grad` for automatic differentiation.\n",
        "\n",
        "While strategies like tensor parallelism and pipeline parallelism for neural networks are more complex (as seen in the full JAX documentation), the building blocks you've learned here are fundamental to them.\n",
        "\n",
        "Continue exploring the JAX documentation to discover even more advanced patterns and unlock the full potential of your multi-device hardware! Happy sharding!"
      ],
      "metadata": {
        "id": "7XYB0wsSQ5Ef"
      }
    }
  ]
}