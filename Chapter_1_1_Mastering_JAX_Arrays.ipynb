{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/oiEKWRlr6ZJ3rQBzCmKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-sahu/jax-sharding-tutorials/blob/main/Project_Aurora_Mastering_JAX_Arrays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Aurora: Mastering JAX Arrays - The Lifeblood of Colossal AI\n",
        "\n",
        "Welcome, Architect, to a foundational briefing for Project Aurora. Our ambition is to construct AI models of unprecedented scale and intelligence. To do this, we must first master the very essence of data within our chosen framework, JAX. It's not enough to just have numbers; we need to understand how JAX represents, manages, and places data across Aurora's vast computational hardware. This is where JAX Arrays come into play – they are far more than simple containers; they are the lifeblood of our models.\n",
        "\n",
        "## The Anatomy of a JAX Array: More Than Just Numbers\n",
        "\n",
        "In Project Aurora, precision and efficiency are paramount. A JAX array, represented by `jax.Array`, is not merely a collection of numbers like a standard Python list or even a NumPy array. It's a sophisticated entity designed for high-performance numerical computation, especially on accelerators like GPUs and TPUs.\n",
        "\n",
        "**Key Characteristics of JAX Arrays:**\n",
        "\n",
        "1.  **Immutability:** Once a JAX array is created, its values cannot be changed in place. Any operation that appears to modify an array actually returns a *new* JAX array. This functional purity is a cornerstone of JAX, enabling cleaner code, easier reasoning about transformations, and powerful compiler optimizations. For Aurora, this means predictability and the ability to safely parallelize operations.\n",
        "\n",
        "2.  **Device Affinity (`DeviceArray`):** This is where JAX arrays truly diverge from their host-bound cousins. A JAX array, particularly when it's a `jax.DeviceArray` (a common type of `jax.Array`), has a \"home.\" It resides physically on a specific computational **device** – a CPU, a GPU, or a TPU. This is crucial because computations involving this array will ideally happen on that same device, minimizing costly data transfers.\n",
        "    * A NumPy array, by contrast, lives in the host computer's RAM, accessible by the CPU.\n",
        "\n",
        "4.  **Asynchronous Execution:** Many operations that create or modify JAX arrays, especially on accelerators, are dispatched asynchronously. This means JAX can tell the device to start work and the Python code can continue running without waiting. This keeps Aurora's systems responsive and efficient. We'll touch on how to manage this later.\n",
        "\n",
        "Let's see these concepts in action."
      ],
      "metadata": {
        "id": "8_ZqbfQMgtHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Default JAX backend: {jax.default_backend()}\")\n",
        "\n",
        "# A standard NumPy array (lives in Host RAM)\n",
        "numpy_arr = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
        "print(f\"\\n--- NumPy Array ---\")\n",
        "print(f\"NumPy array: {numpy_arr}\")\n",
        "print(f\"Type of NumPy array: {type(numpy_arr)}\")\n",
        "\n",
        "# Creating a JAX array from a NumPy array\n",
        "# jnp.array() will typically place it on the default JAX device\n",
        "jax_arr_from_numpy = jnp.array(numpy_arr)\n",
        "print(f\"\\n--- JAX Array (from NumPy) ---\")\n",
        "print(f\"JAX array: {jax_arr_from_numpy}\")\n",
        "print(f\"Type of JAX array: {type(jax_arr_from_numpy)}\")\n",
        "print(f\"Device of JAX array: {jax_arr_from_numpy.device}\")\n",
        "print(f\"Shape of JAX array: {jax_arr_from_numpy.shape}\")\n",
        "print(f\"Dtype of JAX array: {jax_arr_from_numpy.dtype}\")\n",
        "\n",
        "# Creating a JAX array directly from a Python list\n",
        "python_list = [[5.0, 6.0], [7.0, 8.0]]\n",
        "jax_arr_from_list = jnp.array(python_list)\n",
        "print(f\"\\n--- JAX Array (from Python List) ---\")\n",
        "print(f\"JAX array: {jax_arr_from_list}\")\n",
        "print(f\"Device of JAX array: {jax_arr_from_list.device}\")\n",
        "\n",
        "# Immutability in action\n",
        "original_jax_arr = jnp.array([10, 20, 30])\n",
        "print(f\"\\n--- Immutability ---\")\n",
        "print(f\"Original JAX array ({id(original_jax_arr)}): {original_jax_arr}\")\n",
        "modified_jax_arr = original_jax_arr.at[0].set(100) # This creates a NEW array\n",
        "print(f\"Supposedly modified JAX array ({id(modified_jax_arr)}): {modified_jax_arr}\")\n",
        "print(f\"Original JAX array after .at[0].set(100) ({id(original_jax_arr)}): {original_jax_arr}\")\n",
        "# Note the different IDs (in most cases) and that 'original_jax_arr' is unchanged."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOzIcsddg1e_",
        "outputId": "4a501422-01df-4c69-b107-8273b6edd4c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.5.2\n",
            "Default JAX backend: gpu\n",
            "\n",
            "--- NumPy Array ---\n",
            "NumPy array: [1. 2. 3. 4.]\n",
            "Type of NumPy array: <class 'numpy.ndarray'>\n",
            "\n",
            "--- JAX Array (from NumPy) ---\n",
            "JAX array: [1. 2. 3. 4.]\n",
            "Type of JAX array: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
            "Device of JAX array: cuda:0\n",
            "Shape of JAX array: (4,)\n",
            "Dtype of JAX array: float32\n",
            "\n",
            "--- JAX Array (from Python List) ---\n",
            "JAX array: [[5. 6.]\n",
            " [7. 8.]]\n",
            "Device of JAX array: cuda:0\n",
            "\n",
            "--- Immutability ---\n",
            "Original JAX array (766908064): [10 20 30]\n",
            "Supposedly modified JAX array (775258032): [100  20  30]\n",
            "Original JAX array after .at[0].set(100) (766908064): [10 20 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Aurora's architects, understanding that our JAX arrays are device-aware and immutable is the first step to wielding them effectively. The `device` attribute tells us exactly which piece of Aurora's hardware is responsible for this specific piece of data."
      ],
      "metadata": {
        "id": "CbYBsybSg1Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discovering Aurora's Hardware: Where JAX Arrays Reside\n",
        "\n",
        "To intelligently place and manage our JAX arrays, we first need a map of Aurora's computational landscape. JAX provides tools to discover the available devices."
      ],
      "metadata": {
        "id": "7WZevLrAh16m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Discovering Devices ---\")\n",
        "# List all available JAX devices (CPUs, GPUs, TPUs)\n",
        "all_devices = jax.devices()\n",
        "print(f\"All available JAX devices: {all_devices}\")\n",
        "print(f\"Total number of JAX devices: {jax.device_count()}\")\n",
        "\n",
        "# List devices local to the current JAX process\n",
        "# In a typical single-process setup, this is often the same as jax.devices()\n",
        "local_devices = jax.local_devices()\n",
        "print(f\"Local JAX devices: {local_devices}\")\n",
        "print(f\"Number of local JAX devices: {jax.local_device_count()}\")\n",
        "\n",
        "# You can also query devices for a specific backend\n",
        "try:\n",
        "    cpu_devices = jax.devices(\"cpu\")\n",
        "    print(f\"CPU devices: {cpu_devices}\")\n",
        "except:\n",
        "    print(\"CPU backend not explicitly found or no CPU devices listed this way.\")\n",
        "\n",
        "try:\n",
        "    gpu_devices = jax.devices(\"gpu\")\n",
        "    print(f\"GPU devices: {gpu_devices}\")\n",
        "except:\n",
        "    print(\"GPU backend not found or no GPU devices available.\")\n",
        "\n",
        "# The default device is typically the first one in the local_devices list\n",
        "if local_devices:\n",
        "    default_pytree_device = jax.tree_util.tree_leaves(jax_arr_from_list)[0].device\n",
        "    print(f\"The jax_arr_from_list is on device: {default_pytree_device}\") # Same as jax_arr_from_list.device()\n",
        "    print(f\"This is likely the same as local_devices[0]: {local_devices[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJaDYRThhob7",
        "outputId": "373f8089-2a0f-4953-ee11-c051e9a72ad5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Discovering Devices ---\n",
            "All available JAX devices: [CudaDevice(id=0)]\n",
            "Total number of JAX devices: 1\n",
            "Local JAX devices: [CudaDevice(id=0)]\n",
            "Number of local JAX devices: 1\n",
            "CPU devices: [CpuDevice(id=0)]\n",
            "GPU devices: [CudaDevice(id=0)]\n",
            "The jax_arr_from_list is on device: cuda:0\n",
            "This is likely the same as local_devices[0]: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we simply use `jnp.array()`, JAX places the new array on a default device, usually the most capable one it finds (e.g., a GPU or TPU if available, otherwise a CPU). For Aurora's complex models, relying on defaults isn't always optimal. We need explicit control."
      ],
      "metadata": {
        "id": "vxoHs05JiYQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking Command: Explicitly Placing JAX Arrays with `jax.device_put()`\n",
        "\n",
        "This is where Aurora's architects gain true mastery. `jax.device_put()` allows us to dictate precisely which device a JAX array should live on. This is crucial for:\n",
        "\n",
        "* **Performance**: Minimizing data movement by placing arrays on the device where they'll be used.\n",
        "* **Resource Management**: Distributing data across multiple accelerators for parallel processing (the focus of later Aurora briefings on sharding).\n",
        "* **Interfacing**: Moving data from host-based NumPy arrays into the JAX-controlled device memory."
      ],
      "metadata": {
        "id": "j2oAkINtiegD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create some data on the host (CPU RAM)\n",
        "host_blueprint_data = np.random.rand(2, 3).astype(np.float32)\n",
        "print(f\"\\n--- Explicit Placement with device_put ---\")\n",
        "print(f\"Host blueprint data (NumPy array on CPU RAM): \\n{host_blueprint_data}\")\n",
        "\n",
        "# Select target devices (if available)\n",
        "# For Aurora, imagine these are specific processing units in our vast cluster\n",
        "target_cpu = None\n",
        "if cpu_devices: # from previous cell\n",
        "    target_cpu = cpu_devices[0]\n",
        "\n",
        "target_accelerator = None\n",
        "if 'gpu_devices' in locals() and gpu_devices:\n",
        "    target_accelerator = gpu_devices[0]\n",
        "elif local_devices and local_devices[0].platform.lower() != 'cpu': # any non-CPU as accelerator\n",
        "    target_accelerator = local_devices[0]\n",
        "else: # Fallback if no distinct accelerator, use CPU for demonstration\n",
        "    target_accelerator = target_cpu if target_cpu else local_devices[0] if local_devices else None\n",
        "\n",
        "print(f\"Target CPU device for placement: {target_cpu}\")\n",
        "print(f\"Target Accelerator device for placement: {target_accelerator}\")\n",
        "\n",
        "# 1. Host-to-Device (H2D) Transfer: NumPy array to a specific JAX device\n",
        "if target_accelerator:\n",
        "    print(f\"\\nPlacing host data onto Accelerator ({target_accelerator.platform}): {target_accelerator}\")\n",
        "    aurora_data_on_accel = jax.device_put(host_blueprint_data, device=target_accelerator)\n",
        "    print(f\"Aurora data on accelerator: \\n{aurora_data_on_accel}\")\n",
        "    print(f\"Device of aurora_data_on_accel: {aurora_data_on_accel.device}\")\n",
        "    print(f\"Type: {type(aurora_data_on_accel)}\")\n",
        "else:\n",
        "    print(\"\\nNo specific accelerator found to demonstrate H2D, using default placement.\")\n",
        "    aurora_data_on_accel = jax.device_put(host_blueprint_data) # uses default device\n",
        "\n",
        "# 2. Device-to-Device (D2D) Transfer (if distinct devices are available)\n",
        "# Imagine transferring processed data from one Aurora GPU to another, or GPU to CPU JAX device\n",
        "if target_cpu and target_accelerator and target_cpu != target_accelerator:\n",
        "    print(f\"\\nTransferring data from Accelerator ({target_accelerator}) to CPU device ({target_cpu})\")\n",
        "    aurora_data_on_cpu_device = jax.device_put(aurora_data_on_accel, device=target_cpu)\n",
        "    print(f\"Aurora data now on CPU device: \\n{aurora_data_on_cpu_device}\")\n",
        "    print(f\"Device of aurora_data_on_cpu_device: {aurora_data_on_cpu_device.device}\")\n",
        "elif target_cpu == target_accelerator and target_cpu is not None :\n",
        "    print(f\"\\nAccelerator and CPU target are the same JAX device ({target_cpu}), D2D demo for distinct devices not applicable here.\")\n",
        "    # If data is already on the target device, device_put can be a no-op or very fast.\n",
        "    already_on_target_check = jax.device_put(aurora_data_on_accel, target_accelerator)\n",
        "    print(f\"Data put to its own device. Device: {already_on_target_check.device()}. ID of array: {id(already_on_target_check)} vs {id(aurora_data_on_accel)}\")\n",
        "else:\n",
        "    print(\"\\nNot enough distinct devices to demonstrate D2D transfer clearly.\")\n",
        "\n",
        "# device_put also works with JAX arrays already on a device\n",
        "if 'aurora_data_on_accel' in locals() and local_devices and len(local_devices)>0 :\n",
        "    initial_arr = jnp.ones(3) # on default device\n",
        "    print(f\"\\nInitial array on {initial_arr.device}\")\n",
        "    arr_on_specific_dev = jax.device_put(initial_arr, local_devices[0]) # ensure on first device\n",
        "    print(f\"Array explicitly on {arr_on_specific_dev.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dehgA8Mph9vY",
        "outputId": "caa47601-8380-4235-d1c4-ea6142a5f925"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Explicit Placement with device_put ---\n",
            "Host blueprint data (NumPy array on CPU RAM): \n",
            "[[0.22286515 0.93983203 0.92845   ]\n",
            " [0.37479925 0.9169095  0.02802854]]\n",
            "Target CPU device for placement: TFRT_CPU_0\n",
            "Target Accelerator device for placement: cuda:0\n",
            "\n",
            "Placing host data onto Accelerator (gpu): cuda:0\n",
            "Aurora data on accelerator: \n",
            "[[0.22286515 0.93983203 0.92845   ]\n",
            " [0.37479925 0.9169095  0.02802854]]\n",
            "Device of aurora_data_on_accel: cuda:0\n",
            "Type: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
            "\n",
            "Transferring data from Accelerator (cuda:0) to CPU device (TFRT_CPU_0)\n",
            "Aurora data now on CPU device: \n",
            "[[0.22286515 0.93983203 0.92845   ]\n",
            " [0.37479925 0.9169095  0.02802854]]\n",
            "Device of aurora_data_on_cpu_device: TFRT_CPU_0\n",
            "\n",
            "Initial array on cuda:0\n",
            "Array explicitly on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `jax.device_put`, we give Aurora direct instructions. If data is already a JAX array on the target device, `jax.device_put` is often very efficient, potentially just returning the same array. If it needs to move (e.g., host RAM to GPU HBM, or GPU0 HBM to GPU1 HBM), a transfer occurs."
      ],
      "metadata": {
        "id": "9K7mwqdlkHmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving Aurora's Discoveries: jax.device_get()\n",
        "\n",
        "Sometimes, after complex computations on Aurora's accelerators, we need to bring results back to the host CPU – perhaps for saving to disk, for visualization with libraries like Matplotlib, or for parts of the application logic that run in standard Python/NumPy. For this, we use `jax.device_get()`."
      ],
      "metadata": {
        "id": "Qlbtg3xhkRdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Retrieving Data with device_get ---\")\n",
        "# Assuming aurora_data_on_accel is a JAX array on an accelerator (from previous step)\n",
        "if 'aurora_data_on_accel' not in locals(): # Create one if it doesn't exist\n",
        "    aurora_data_on_accel = jnp.array([[10.,20.],[30.,40.]], device=target_accelerator if target_accelerator else local_devices[0])\n",
        "    print(\"(Recreated aurora_data_on_accel for this section)\")\n",
        "\n",
        "\n",
        "print(f\"Data on accelerator ({aurora_data_on_accel.device}): \\n{aurora_data_on_accel}\")\n",
        "print(f\"Type before get: {type(aurora_data_on_accel)}\")\n",
        "\n",
        "# Retrieve data from the device to the host CPU RAM as a NumPy array\n",
        "retrieved_blueprint = jax.device_get(aurora_data_on_accel)\n",
        "\n",
        "print(f\"\\nRetrieved blueprint (now on host CPU RAM): \\n{retrieved_blueprint}\")\n",
        "print(f\"Type after get: {type(retrieved_blueprint)}\") # Should be <class 'numpy.ndarray'>\n",
        "\n",
        "# Now you can use standard NumPy operations or save it\n",
        "sum_on_host = np.sum(retrieved_blueprint)\n",
        "print(f\"Sum computed on host using NumPy: {sum_on_host}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EetaEzA6jD2A",
        "outputId": "a7e362bb-cb83-4493-d78a-695a80e2d990"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Retrieving Data with device_get ---\n",
            "Data on accelerator (cuda:0): \n",
            "[[0.22286515 0.93983203 0.92845   ]\n",
            " [0.37479925 0.9169095  0.02802854]]\n",
            "Type before get: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
            "\n",
            "Retrieved blueprint (now on host CPU RAM): \n",
            "[[0.22286515 0.93983203 0.92845   ]\n",
            " [0.37479925 0.9169095  0.02802854]]\n",
            "Type after get: <class 'numpy.ndarray'>\n",
            "Sum computed on host using NumPy: 3.410884380340576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A critical point for Aurora's performance: `jax.device_get()` is a synchronous operation. Your Python program will pause and wait until the data has been fully copied from the device to the host. Frequent, unnecessary `device_get` calls within performance-critical loops can become serious bottlenecks. For Aurora, we aim to keep data on devices as long as possible, only retrieving it when absolutely necessary."
      ],
      "metadata": {
        "id": "53b9XyDek0Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Pulse of Aurora: Asynchronous Execution and `block_until_ready()`\n",
        "\n",
        "As mentioned, JAX operations, particularly those targeting accelerators, often execute asynchronously. The Python host tells the device \"do this computation,\" and JAX quickly returns an `Array` *future* or *handle* to the result, even if the computation isn't finished yet. The host can then continue queueing more work.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JU8vIc54k_Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Asynchronous Execution & Blocking ---\")\n",
        "# Let's use a JAX array that is hopefully on an accelerator\n",
        "data_for_op = jnp.arange(1_000_000, dtype=jnp.float32).reshape(1000,1000)\n",
        "if target_accelerator:\n",
        "  data_for_op = jax.device_put(data_for_op, target_accelerator)\n",
        "print(f\"(Created large data_for_op on {data_for_op.device} for async demo)\")\n",
        "\n",
        "# This operation is dispatched to the device. Python might continue before it's done.\n",
        "# For very fast ops, the effect might be hard to see without proper profiling.\n",
        "print(\"Dispatching a potentially large computation...\")\n",
        "result_future = jnp.dot(data_for_op, data_for_op.T) # Matrix multiplication\n",
        "print(f\"Python host sees a result 'future' of type: {type(result_future)}\")\n",
        "print(f\"Result future is on device: {result_future.device}\")\n",
        "\n",
        "# If we need to ensure the computation is complete before proceeding (e.g., timing,\n",
        "# or using the result outside JAX), we use .block_until_ready()\n",
        "result_future.block_until_ready()\n",
        "print(\"Computation is now guaranteed to be complete.\")\n",
        "\n",
        "# Accessing the result (e.g., printing or jax.device_get) will also implicitly block.\n",
        "print(f\"First element of result: {result_future[0,0]}\") # This would block until result[0,0] is ready\n",
        "# (Don't print large matrices usually)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezOBNzyEkhHK",
        "outputId": "dcf5cd58-d8c6-4873-d38b-558f8fb944d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Asynchronous Execution & Blocking ---\n",
            "(Created large data_for_op on cuda:0 for async demo)\n",
            "Dispatching a potentially large computation...\n",
            "Python host sees a result 'future' of type: <class 'jaxlib.xla_extension.ArrayImpl'>\n",
            "Result future is on device: cuda:0\n",
            "Computation is now guaranteed to be complete.\n",
            "First element of result: 332833216.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Aurora's architects, `array.block_until_ready()` is the tool to synchronize the host with the device. It's essential when timing operations accurately or when external actions depend on the result being available."
      ],
      "metadata": {
        "id": "V6Y2TdwqmAWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Engine Room: A Brief Word on XLA\n",
        "\n",
        "Beneath the elegant JAX NumPy API lies a powerful compiler: **XLA (Accelerated Linear Algebra)**. When you JIT-compile JAX functions, or even when you perform operations on JAX arrays, JAX translates this into an XLA computation graph. XLA then optimizes this graph and compiles it into highly efficient machine code for the target CPU, GPU, or TPU.\n",
        "\n",
        "JAX arrays, are JAX's way of representing data that XLA manages on these devices. XLA handles the low-level details of memory allocation, kernel launching, and often decides the optimal physical layout of your data in device memory for maximum performance. While we usually interact with JAX APIs, knowing XLA is the engine empowers us to understand why certain JAX features (like JIT compilation) are so effective."
      ],
      "metadata": {
        "id": "jc80RZFRmJGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundations Laid\n",
        "Architect, you now have a much deeper understanding of JAX arrays – Aurora's fundamental data building blocks. You know they are immutable, device-aware entities. You can query Aurora's hardware landscape, explicitly command where data resides using jax.device_put, retrieve it using jax.device_get, and appreciate the asynchronous nature of JAX's execution model.\n",
        "\n",
        "This mastery of individual data primitives is the bedrock upon which all of Aurora's distributed computing strategies will be built. Next, we will explore how to take these JAX arrays and the computations upon them, and distribute them across many devices working in concert – the true path to colossal AI."
      ],
      "metadata": {
        "id": "0wrhpt6VmnuX"
      }
    }
  ]
}